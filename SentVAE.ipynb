{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcYW7MDKScQ6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMktCZC3Vwyg"
   },
   "source": [
    "In this notebook you will work with a deep generative language model that maps sentences from a continuous latent space. We will use text data in English and pytorch. \n",
    "\n",
    "The first section concerns data manipulation and data loading classes necessary for our implementation. You do not need to modify anything in this part of the code, but it is useful to read through as it might illustrate some good practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ca_L1zfDScRC"
   },
   "source": [
    "Let's first download the Penn Treebank dataset that we will be using for this notebook: these are the sentences in the PTB English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pYl0oQJVScRE",
    "outputId": "c3d0d881-3bff-44d4-ade7-8d1070276a32"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jhcross/span-parser/master/data/\"\n",
    "train_file = \"02-21.10way.clean\"\n",
    "val_file = \"22.auto.clean\"\n",
    "test_file = \"23.auto.clean\"\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print(\"Downloading data files...\")\n",
    "    if not os.path.isfile(train_file):\n",
    "        urllib.request.urlretrieve(url + train_file, filename=train_file)\n",
    "    if not os.path.isfile(val_file):\n",
    "        urllib.request.urlretrieve(url + val_file, filename=val_file)\n",
    "    if not os.path.isfile(test_file):\n",
    "        urllib.request.urlretrieve(url + test_file, filename=test_file)\n",
    "    print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPlLBwayScRN"
   },
   "source": [
    "# Data\n",
    "\n",
    "In order to work with text data, we need to transform the text into something that our algorithms can work with. The first step of this process is converting words into word ids. We do this by constructing a vocabulary from the data, assigning a new word id to each new word it encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4wtwvwaScRP"
   },
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<unk>\"  # 0\n",
    "PAD_TOKEN = \"<pad>\"  # 1\n",
    "SOS_TOKEN = \"<s>\"    # 2\n",
    "EOS_TOKEN = \"</s>\"   # 3\n",
    "\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "    \"\"\"\n",
    "    Removes the parse trees in the PTB dataset.\n",
    "    \"\"\"\n",
    "    return \" \".join(re.findall(r\"\\([A-Z]* ([^\\(\\)]+)\\)\", s))\n",
    "\n",
    "def iget_next_line(filepath):\n",
    "    with open(filepath) as f:\n",
    "        yield from f\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.idx_to_word = {0: UNK_TOKEN, 1: PAD_TOKEN, 2: SOS_TOKEN, 3: EOS_TOKEN}\n",
    "        self.word_to_idx = {UNK_TOKEN: 0, PAD_TOKEN: 1, SOS_TOKEN: 2, EOS_TOKEN: 3}\n",
    "        self.word_freqs = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.word_to_idx[key] if key in self.word_to_idx else self.word_to_idx[UNK_TOKEN]\n",
    "    \n",
    "    def word(self, idx):\n",
    "        return self.idx_to_word[idx]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.word_to_idx)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_data(filenames):\n",
    "        \"\"\"\n",
    "            Creates a vocabulary from a list of data files. It assumes that the data files have been\n",
    "            tokenized and pre-processed beforehand.\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary()\n",
    "        for filename in filenames:\n",
    "            for line in iget_next_line(filename):\n",
    "                # Strip whitespace and the newline symbol.\n",
    "                line = tokens_from_treestring(line.strip())\n",
    "\n",
    "                # Split the sentences into words and assign word ids to each\n",
    "                # new word it encounters.\n",
    "                for word in line.split():\n",
    "                    if word not in vocab.word_to_idx:\n",
    "                        idx = len(vocab.word_to_idx)\n",
    "                        vocab.word_to_idx[word] = idx\n",
    "                        vocab.idx_to_word[idx] = word\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In an Oct. 19 review of The Misanthrope at Chicago 's Goodman Theatre Revitalized Classics Take the Stage in Windy City Leisure & Arts the role of Celimene played by Kim Cattrall was mistakenly attributed to Christina Haag\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tokens_from_treestring func\n",
    "line = next(iget_next_line(train_file))\n",
    "tokens_from_treestring(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5yGmXEQ2TQSJ",
    "outputId": "21cf0db6-51ee-4df3-be1a-f4b853985c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing vocabulary...\n",
      "Constructed a vocabulary of 45233 word types\n"
     ]
    }
   ],
   "source": [
    "# Construct a vocabulary from the training and validation data.\n",
    "print(\"Constructing vocabulary...\")\n",
    "vocab = Vocabulary.from_data([train_file, val_file])\n",
    "print(\"Constructed a vocabulary of {} word types\".format(vocab.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jLP1Yy2_TtzR",
    "outputId": "9babc256-60e2-43f8-9ec7-2eff8d4e937d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 20\n",
      "thing 630\n",
      "VAE 0\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print('the', vocab['the'])\n",
    "print('thing', vocab['thing'])\n",
    "print('VAE', vocab['VAE'])  # something UNKNOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Wwdq1DtScRX"
   },
   "source": [
    "We also need to load the data files into memory. We create a simple class `TextDataset` that stores the data as a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYC5fXfIScRa"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        A simple class that loads a list of sentences into memory from a text file,\n",
    "        split by newlines. This does not do any memory optimisation, \n",
    "        so if your dataset is very large, you might want to use an alternative \n",
    "        class.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_file, max_len=30):\n",
    "        self.data = []\n",
    "        for line in iget_next_line(text_file):\n",
    "            sentence = tokens_from_treestring(line.strip())\n",
    "            if len(sentence.split()) <= max_len:\n",
    "                self.data.append(sentence)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "SehavEDQTfIe",
    "outputId": "c03d43e8-bce6-421e-ca79-19b5206fe27f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training data: \"They believe the media including Batibot have played a crucial role\"\n",
      "Sample from validation data: \"Foreign stock markets which kicked off Black Monday with a huge selling spree began the day off by relatively modest amounts\"\n",
      "Sample from test data: \"Executives at Prudential-Bache Securities Inc. a Backer Spielvogel client that is reviewing account declined comment\"\n"
     ]
    }
   ],
   "source": [
    "# Load the training, validation, and test datasets into memory.\n",
    "train_dataset = TextDataset(train_file)\n",
    "val_dataset = TextDataset(val_file)\n",
    "test_dataset = TextDataset(test_file)\n",
    "\n",
    "# Print some samples from the data:\n",
    "print(\"Sample from training data: \\\"%s\\\"\" % train_dataset[np.random.choice(len(train_dataset))])\n",
    "print(\"Sample from validation data: \\\"%s\\\"\" % val_dataset[np.random.choice(len(val_dataset))])\n",
    "print(\"Sample from test data: \\\"%s\\\"\" % test_dataset[np.random.choice(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNQ1TG-3ScRh"
   },
   "source": [
    "Now it's time to write a function that converts a sentence into a list of word ids using the vocabulary we created before. This function is `create_batch` in the code cell below. This function creates a batch from a list of sentences, and makes sure that each sentence starts with a start-of-sentence symbol and ends with an end-of-sentence symbol. Because not all sentences are of equal length in a certain batch, sentences are padded with padding symbols so that they match the length of the largest sentence in the batch. The function returns an input batch, an output batch, a mask of 1s for words and 0s for padding symbols, and the sequence lengths of each sentence in the batch. The output batch is shifted by one word, to reflect the predictions that the model is expected to make. For example, for a sentence\n",
    "\\begin{align}\n",
    "    \\text{The dog runs .}\n",
    "\\end{align}\n",
    "the input sequence is\n",
    "\\begin{align}\n",
    "    \\text{SOS The dog runs .}\n",
    "\\end{align}\n",
    "and the output sequence is\n",
    "\\begin{align}\n",
    "    \\text{The dog runs . EOS}\n",
    "\\end{align}\n",
    "\n",
    "You can see the output is shifted wrt the input, that's because we will be computing a distribution for the next word in context of its prefix, and that's why we need to shift the sequence this way.\n",
    "\n",
    "\n",
    "Lastly, we create an inverse function `batch_to_sentences` that recovers the list of sentences from a padded batch of word ids to use during test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcYfsIaBScRj"
   },
   "outputs": [],
   "source": [
    "def create_batch(sentences, vocab, device, word_dropout=0.):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences to a padded batch of word ids. Returns\n",
    "    an input batch, an output batch shifted by one, a sequence mask over\n",
    "    the input batch, and a tensor containing the sequence length of each\n",
    "    batch element.\n",
    "    :param sentences: a list of sentences, each a list of token ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :param device: \n",
    "    :param word_dropout: rate at which we omit words from the context (input)\n",
    "    :returns: a batch of padded inputs, a batch of padded outputs, mask, lengths\n",
    "    \"\"\"\n",
    "    tok = np.array([[SOS_TOKEN] + sent.split() + [EOS_TOKEN] for sent in sentences])\n",
    "    seq_lengths = [len(sent)-1 for sent in tok]\n",
    "    max_len = max(seq_lengths)\n",
    "    pad_id = vocab[PAD_TOKEN]\n",
    "    pad_id_input = [\n",
    "        [vocab[sent[t]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, sent in enumerate(tok)]\n",
    "    \n",
    "    # Replace words of the input with <unk> with p = word_dropout.\n",
    "    # it is possible to check also dropping only after <s> token\n",
    "    if word_dropout > 0.:\n",
    "        unk_id = vocab[UNK_TOKEN]\n",
    "        word_drop = [\n",
    "            [unk_id if (np.random.random() < word_dropout and t < seq_lengths[idx]) else word_ids[t] for t in range(max_len)] \n",
    "                for idx, word_ids in enumerate(pad_id_input)]\n",
    "        pad_id_input = word_drop\n",
    "    \n",
    "    # The output batch is shifted by 1.\n",
    "    pad_id_output = [\n",
    "        [vocab[sent[t+1]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, sent in enumerate(tok)]\n",
    "    \n",
    "    # Convert everything to PyTorch tensors.\n",
    "    batch_input = torch.tensor(pad_id_input)\n",
    "    batch_output = torch.tensor(pad_id_output)\n",
    "    seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
    "    seq_length = torch.tensor(seq_lengths)\n",
    "    \n",
    "    # Move all tensors to the given device.\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_output = batch_output.to(device)\n",
    "    seq_mask = seq_mask.to(device)\n",
    "    seq_length = seq_length.to(device)\n",
    "    \n",
    "    return batch_input, batch_output, seq_mask, seq_length\n",
    "\n",
    "\n",
    "def batch_to_sentences(tensors, vocab: Vocabulary):\n",
    "    \"\"\"\n",
    "    Converts a batch of word ids back to sentences.\n",
    "    :param tensors: [B, T] word ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :returns: an array of strings (each a sentence).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    batch_size = tensors.size(0)\n",
    "    for idx in range(batch_size):\n",
    "        sentence = [vocab.word(t.item()) for t in tensors[idx,:]]\n",
    "        \n",
    "        # Filter out the start-of-sentence and padding tokens.\n",
    "        sentence = list(filter(lambda t: t != PAD_TOKEN and t != SOS_TOKEN, sentence))\n",
    "        \n",
    "        # Remove the end-of-sentence token and all tokens following it.\n",
    "        if EOS_TOKEN in sentence:\n",
    "            sentence = sentence[:sentence.index(EOS_TOKEN)]\n",
    "            \n",
    "        sentences.append(\" \".join(sentence))\n",
    "    return np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCES [\"No it was n't Black Monday\", 'The finger-pointing has already begun']\n",
      "INP BATCH\n",
      " tensor([[    2,  1617,    48,    34,   102,  5359,  1715],\n",
      "        [    2,    10, 34227,   101,   191,  7178,     1]])\n",
      "OUT BATCH\n",
      " tensor([[ 1617,    48,    34,   102,  5359,  1715,     3],\n",
      "        [   10, 34227,   101,   191,  7178,     3,     1]])\n",
      "SEQ MASK\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]], dtype=torch.uint8)\n",
      "SEQ LENGTH\n",
      " tensor([7, 6])\n",
      "\n",
      "WORD DROPOUT = 0.1\n",
      "SENTENCES [\"No it was n't Black Monday\", 'The finger-pointing has already begun']\n",
      "INP BATCH\n",
      " tensor([[    2,  1617,    48,    34,     0,  5359,  1715],\n",
      "        [    2,    10, 34227,   101,   191,  7178,     1]])\n",
      "OUT BATCH\n",
      " tensor([[ 1617,    48,    34,   102,  5359,  1715,     3],\n",
      "        [   10, 34227,   101,   191,  7178,     3,     1]])\n",
      "SEQ MASK\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]], dtype=torch.uint8)\n",
      "SEQ LENGTH\n",
      " tensor([7, 6])\n"
     ]
    }
   ],
   "source": [
    "# check create_batch & batch_to_sentences funcs\n",
    "sentences = [\n",
    "    test_dataset[0],\n",
    "    test_dataset[5]\n",
    "]\n",
    "\n",
    "batch_input, batch_output, seq_mask, seq_length = create_batch(sentences, vocab, device, 0)\n",
    "print('SENTENCES', sentences)\n",
    "print('INP BATCH\\n', batch_input.cpu())\n",
    "print('OUT BATCH\\n', batch_output.cpu())\n",
    "print('SEQ MASK\\n', seq_mask.cpu())\n",
    "print('SEQ LENGTH\\n', seq_length.cpu())\n",
    "\n",
    "assert (sentences == batch_to_sentences(batch_input, vocab)).all()\n",
    "assert (sentences == batch_to_sentences(batch_output, vocab)).all()\n",
    "\n",
    "word_dropout = 0.1\n",
    "print('\\nWORD DROPOUT = {}'.format(word_dropout))\n",
    "batch_input, batch_output, seq_mask, seq_length = create_batch(sentences, vocab, device, word_dropout)\n",
    "print('SENTENCES', sentences)\n",
    "print('INP BATCH\\n', batch_input.cpu())\n",
    "print('OUT BATCH\\n', batch_output.cpu())\n",
    "print('SEQ MASK\\n', seq_mask.cpu())\n",
    "print('SEQ LENGTH\\n', seq_length.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7R5_4wwScRq"
   },
   "source": [
    "In PyTorch the RNN functions expect inputs to be sorted from long sentences to shorter ones. Therefore we create a simple wrapper class for the DataLoader class that sorts sentences from long to short:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xjtwes5iScRs"
   },
   "outputs": [],
   "source": [
    "class SortingTextDataLoader(object):\n",
    "    \"\"\"\n",
    "    A wrapper for the DataLoader class that sorts a list of sentences by their\n",
    "    lengths in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.it = iter(dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        sentences = None\n",
    "        for s in self.it:\n",
    "            sentences = s\n",
    "            break\n",
    "\n",
    "        if sentences is None:\n",
    "            self.it = iter(self.dataloader)\n",
    "            raise StopIteration\n",
    "        \n",
    "        sentences = np.array(sentences)\n",
    "        sort_keys = sorted(range(len(sentences)), \n",
    "                           key=lambda idx: len(sentences[idx].split()), \n",
    "                           reverse=True)\n",
    "        sorted_sentences = sentences[sort_keys]\n",
    "        return sorted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some circuit breakers installed after the October 1987 crash failed first test t 25\n",
      "Big investment banks refused to step up to the plate to support the beleaguered  24\n",
      "Seven Big Board stocks UAL AMR BankAmerica Walt Disney Capital Cities\\/ABC Phili 22\n",
      "Heavy selling of Standard & Poor 's 500-stock index futures in Chicago relentles 16\n",
      "No it was n't Black Monday 6\n"
     ]
    }
   ],
   "source": [
    "dummy_dataloader = SortingTextDataLoader(DataLoader(test_dataset, batch_size=5))\n",
    "sents = next(dummy_dataloader)\n",
    "for sent in sents:\n",
    "    print(sent[:80], len(sent.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkrOx_-NScRw"
   },
   "source": [
    "# Model\n",
    "\n",
    "## Deterministic language model\n",
    "\n",
    "In language modelling, we model a sentence $x = \\langle x_1, \\ldots, x_n \\rangle$  of length $n = |x|$ as a sequence of categorical draws:\n",
    "\n",
    "\\begin{align}\n",
    "X_i|x_{<i} & \\sim \\text{Cat}(f(x_{<i}; \\theta)) \n",
    "& i = 1, \\ldots, n \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we use $x_{<i}$ to denote a (possibly empty) prefix string, and thus the model makes no Markov assumption. We map from the conditioning context, the prefix $x_{<i}$, to the categorical parameters (a $v$-dimensional probability vector, where $v$ denotes the size of the vocabulary of the language) using a fixed neural network architecture whose parameters we collectively denote by $\\theta$.\n",
    "\n",
    "This assigns the following likelihood to the sentence\n",
    "\\begin{align}\n",
    "    P(x|\\theta) &= \\prod_{i=1}^n P(x_i|x_{<i}, \\theta) \\\\\n",
    "    &= \\prod_{i=1}^n \\text{Cat}(x_i|f(x_{<i}; \\theta))  \n",
    "\\end{align}\n",
    "where the categorical pmf is $\\text{Cat}(k|\\pi) = \\prod_{j=1}^v \\pi_j^{[k=j]} = \\pi_k$. \n",
    "\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{x^{(1)}, \\ldots, x^{(N)}\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(x^{(k)}| \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\sum_{i=1}^{|x^{(k)}|} \\log \\text{Cat}(x^{(k)}_i|f(x^{(k)}_{<i}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(x^{(k)}|\\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(x^{(k)}| \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(x^{(K)}| \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(x^{(k_m)}|\\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "\n",
    "An example design for $f$ is:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
    "\\mathbf h_0 &\\in \\theta_{\\text{rnn}} \\\\\n",
    "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
    "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
    "\\end{align}\n",
    "where \n",
    "* $\\text{emb}$ is a fixed embedding layer with parameters $\\theta_{\\text{emb}}$;\n",
    "* $\\text{rnn}$ is a recurrent architecture with parameters $\\theta_{\\text{rnn}}$, e.g. an LSTM or GRU, and $\\mathbf h_0$ is part of the architecture's parameters;\n",
    "* $\\text{dense}_v$ is a dense layer with $v$ outputs (vocabulary size) and parameters $\\theta_{\\text{out}}$.\n",
    "\n",
    "\n",
    "\n",
    "In what follows we show how to extend this model with a continuous latent sentence embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhO8rD2DYuar"
   },
   "source": [
    "## Deep generative language model\n",
    "\n",
    "We want to model a sentence $x$ as a draw from the marginal of deep generative model $p(z, x|\\theta) = p(z)P(x|z, \\theta)$. Note that we use $p(\\cdot)$ for pdfs and $P(\\cdot)$ for pmfs.\n",
    "\n",
    "\n",
    "### Generative model\n",
    "\n",
    "The generative story is:\n",
    "\\begin{align}\n",
    "    Z & \\sim \\mathcal N(0, I) \\\\\n",
    "    X_i | z, x_{<i} &\\sim \\text{Cat}(f(z, x_{<i}; \\theta)) & i=1, \\ldots, n\n",
    "\\end{align}\n",
    "where $z \\in \\mathbb R^D$ and  we impose a standard Gaussian prior on latent space. Other choices of prior can induce interesting properties in latent space, however, in this notebook, we use the Gaussian for its simplicity.\n",
    "\n",
    "It is easy to design $f$ by a simple modification of the deterministic design shown before:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
    "\\mathbf h_0 &= \\tanh(\\text{dense}(z; \\theta_{\\text{init}})) \\\\\n",
    "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
    "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
    "\\end{align}\n",
    "where we just initialise the recurrent cell using $z$. Note we could also use $z$ in other places, for example, as additional input to every update of the recurrent cell $\\mathbf h_i = \\text{rnn}(\\mathbf h_{i-1}, [\\mathbf x_{i-1}, z])$. This is an architecture choice which like many others can only be judged empirically or on the basis of practical convenience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwB7igyXg8uU"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "The marginal likelihood, necessary for parameter estimation, is now no longer tractable:\n",
    "\\begin{align}\n",
    "P(x|\\theta) &= \\int p(z)P(x|z, \\theta) \\text{d}z \\\\\n",
    "&= \\int \\mathcal N(z|0, I)\\prod_{i=1}^n \\text{Cat}(x_i|f(z,x_{<i}; \\theta) )\\text{d}z \n",
    "\\end{align}\n",
    "the intractability is clear as even if $f$ would be linear (recall it isn't) the Gaussian and the Categorical are not conjugate and the integration has no closed-form solution.\n",
    "\n",
    "We turn to variational inference and derive a lowerbound $\\mathcal E(\\theta, \\lambda|\\mathcal D)$ on the log-likelihood function\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal E(\\theta, \\lambda|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\mathcal E_k(\\theta, \\lambda|x^{(k)}) \n",
    "\\end{align}\n",
    "\n",
    "which for a single datapoint $x$ is\n",
    "\\begin{align}\n",
    "    \\mathcal E(\\theta, \\lambda|x) &= \\mathbb{E}_{q(z|x)}\\left[\\log P(x|z, \\theta)\\right] - \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)\\\\\n",
    "\\end{align}\n",
    "where we have introduce an independently parameterised auxiliary distribution $q(z|x, \\lambda)$. The distribution $q$ which maximises this *evidence lowerbound* (ELBO) is also the distribution that minimises \n",
    "\\begin{align}\n",
    "\\text{KL}(q(z|x, \\lambda)||p(z|x, \\theta)) = \\mathbb E_{q(z|x, \\lambda)}\\left[ \\frac{q(z|x, \\lambda)}{p(z|x, \\theta)}\\right]\n",
    "\\end{align}\n",
    " where $p(z|x, \\theta) = \\frac{p(x, z|\\theta)}{p(x|\\theta)}$ is our intractable true posterior. For that reason, we think of $q(z|x, \\lambda)$ as an *approximate posterior*. \n",
    " \n",
    " The approximate posterior is an independent model of the latent variable given the data, for that reason we also call it an *inference model*. \n",
    " In this notebook, our inference model will be a diagonal Gaussian, to make sure that we cover the sample space of our latent variable. A diagonal Gaussian models each component of the latent representation independently, and is an example of mean field (MF) inference:\n",
    " \n",
    " \\begin{align}\n",
    "    q(z|x, \\lambda) &\\overset{\\text{MF}}{=} \\prod_{d=1}^D \\mathcal N(z_d|\\mu_d(x; \\lambda); \\sigma_d(x; \\lambda)^2) \\\\\n",
    "    &= \\mathcal N(z|\\mu(x; \\lambda), \\text{diag}(\\sigma(x; \\lambda)^2))\n",
    " \\end{align}\n",
    " \n",
    " where we employ neural network architectures to map from the sentence $x$ to a vector of locations (in $\\mathbb R^D$) and a vector of scales (in $\\mathbb R^D_{>0}$).\n",
    " \n",
    " \n",
    "For this choice, the KL term in the ELBO is tractable:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right) &= \\sum_{d=1}^D \\text{KL}\\left(q(z_d|x, \\lambda)||p(z_d)\\right) \\\\\n",
    "&= \\sum_{d=1}^D \\text{KL}\\left(\\mathcal N(u_d, s^2)|| \\mathcal N(0,1)\\right) \\\\\n",
    "&= - \\frac{1}{2} \\sum_{d=1}^D \\left(1 + \\log s_d^2 - u_d^2 - s_d^2 \\right)\n",
    "\\end{align}\n",
    "where $u_d = \\mu_d(x; \\lambda)$ and $s_d = \\sigma_d(x; \\lambda)$.\n",
    "\n",
    " \n",
    "Here's an example design for our inference model:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\lambda_{\\text{emb}}) \\\\\n",
    "\\mathbf f_i &= \\text{rnn}(\\mathbf f_{i-1}, \\mathbf x_{i}; \\lambda_{\\text{fwd}}) \\\\\n",
    "\\mathbf b_i &= \\text{rnn}(\\mathbf b_{i+1}, \\mathbf x_{i}; \\lambda_{\\text{bwd}}) \\\\\n",
    "\\mathbf h &= \\text{dense}([\\mathbf f_{n}, \\mathbf b_1]; \\lambda_{\\text{hid}}) \\\\\n",
    "\\mu(x; \\lambda) &= \\text{dense}(\\mathbf h; \\lambda_{\\text{loc}})\\\\\n",
    "\\sigma(x; \\lambda) &= \\text{softplus}(\\text{dense}(\\mathbf h; \\lambda_{\\text{scale}}))\n",
    "\\end{align}\n",
    "\n",
    "where we use the $\\text{softplus}$ activation to make sure our scales are strictly positive. Note that $\\exp$ would also do that job, though $\\text{softplus}$ is assymptotically linear with its input, which gives us better gradient dynamics.\n",
    " \n",
    "Because we have neural networks compute the diagonal Gaussian parameters for us, we call this *amortised* mean field inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8BTCCPVOKp_"
   },
   "source": [
    "### Gradient estimation\n",
    "\n",
    "We have to obtain gradients of the ELBO\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta  \\mathcal E(\\theta, \\lambda|x) &=  \\mathbb{E}_{q(z|x)}\\left[\\nabla_\\theta \\log P(x|z, \\theta)\\right] - \\underbrace{\\nabla_\\theta \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)}_{=0}\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda  \\mathcal E(\\theta, \\lambda|x) &= \\nabla_\\lambda\\mathbb{E}_{q(z|x)}\\left[\\log P(x|z, \\theta)\\right] - \\nabla_\\lambda \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)\n",
    "\\end{align}\n",
    "\n",
    "Clearly, the gradient for the generative network is easy to estimate using MC, but the gradient for the inference network is more complicated because we don't have an expected gradient, rather the gradient of an expected value.\n",
    "\n",
    "But recall, that we chose a Gaussian for approximate posterior, and Gaussians are what we call a location-scale family. Every Gaussian variable can be *re-expressed* or  **reparameterised** in terms of the standard Gaussian (a distribution with fixed parameters), that is:\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon = \\frac{z - \\mu(x; \\sigma)}{\\sigma(x; \\lambda)} &\\sim \\mathcal N(0, I) \\\\\n",
    "z = \\mu(x; \\lambda) + \\sigma(x;\\lambda) \\odot \\epsilon &\\sim \\mathcal N(\\mu(x; \\lambda), \\text{diag}(\\sigma(x; \\lambda)^2))\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is distributed by a $D$-dimensional standard Gaussian and $\\odot$ denotes elementwise multiplication.\n",
    "\n",
    "This means we can rewrite the ELBO as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\lambda|x) &= \\mathbb{E}_{\\epsilon \\sim \\mathcal N(0, I)}\\left[\\log P(x|z=\\mu(x; \\lambda) + \\sigma(x; \\lambda) \\odot \\epsilon, \\theta)\\right] - \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right) \n",
    "\\end{align}\n",
    "\n",
    "and though we could also rewrite the KL term, we will leave as is because it is tractable to compute.\n",
    "\n",
    "Now our gradients are both very simple:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal E(\\theta, \\lambda|x) &= \\mathbb{E}_{\\epsilon \\sim \\mathcal N(0, I)}\\left[\\nabla_\\theta \\log P(x|z=\\mu(x; \\lambda) + \\sigma(x; \\lambda) \\odot \\epsilon, \\theta)\\right] - \\underbrace{\\nabla_\\theta \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)}_{=0} \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(x|z^{(s)}, \\theta) \\\\\n",
    "&\\text{where }z^{(s)} = \\mu(x; \\lambda) + \\sigma(x; \\lambda) \\odot \\epsilon^{(s)}\\\\\n",
    "&\\text{and }\\epsilon^{(s)} \\sim \\mathcal N(0, I)\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal E(\\theta, \\lambda|x) &=\\nabla_\\lambda \\mathbb{E}_{\\epsilon \\sim \\mathcal N(0, I)}\\left[\\nabla_\\lambda \\log P(x|z=\\mu(x; \\lambda) + \\sigma(x; \\lambda) \\odot \\epsilon, \\theta)\\right] - \\nabla_\\lambda \\underbrace{\\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)}_{\\text{tractable}} \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S \\nabla_\\lambda \\log P(x|z^{(s)}, \\theta)\\right) -  \\nabla_\\lambda \\underbrace{\\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)}_{\\text{tractable}}\\\\\n",
    "&\\text{where }z^{(s)} = \\mu(x; \\lambda) + \\sigma(x; \\lambda) \\odot \\epsilon^{(s)}\\\\\n",
    "&\\text{and }\\epsilon^{(s)} \\sim \\mathcal N(0, I)\n",
    "\\end{align}\n",
    "\n",
    "and note how both, but especially the second, use this notion of *reparameterised sample* to make all sources of stochasticity independent of the parameters of the network.\n",
    "\n",
    "Gradient estimates of this sort are known in the literature as *reparameterised gradients* and this makes our model an instance of what is known in the literature as a *variational auto-encoder* (VAE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6lhOGrkTNA3"
   },
   "source": [
    "\n",
    "## Implementation\n",
    "\n",
    "We start by implementing our generative model, which requires implementing a Gaussian/Normal distribution:\n",
    "\n",
    "Check the [wikipedia page about Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) for information such as the functional form of the pdf. \n",
    "\n",
    "You will need the KL divergence for univariate Normal distributions:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{KL}\\left(\\mathcal N(\\mu_1, \\sigma_1^2) || \\mathcal N(\\mu_2, \\sigma_2^2)\\right)\n",
    "&= \\frac{1}{2 \\sigma_2^2} \\left((\\mu_1 - \\mu_2)^2 + \\sigma_1^2 - \\sigma_2^2\\right) + \\log \\frac{\\sigma_2}{\\sigma_1}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITzvi_ovpZUy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Normal(object):\n",
    "    \"\"\"\n",
    "    This is a normal distribution\n",
    "        N(u, s^2)\n",
    "    thus specified by a location u and a strictly positive scale.\n",
    "    \n",
    "    This class can hold a collection of D independent Gaussian variables\n",
    "        by having a D-dimensional vector of locations and \n",
    "        a D-dimensinal vector of scales.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loc, scale):\n",
    "        \"\"\"\n",
    "        :param loc: a tensor of locations (real numbers)\n",
    "        :param scale: a tensor of scales (strictly positive real numbers)\n",
    "        \"\"\"\n",
    "        self.loc, self.scale = loc, scale\n",
    "    \n",
    "    def mean(self):\n",
    "        \"\"\"For Gaussians this is the location\"\"\"\n",
    "        return self.loc\n",
    "    \n",
    "    def std(self):\n",
    "        \"\"\"For Gaussians this is the scale\"\"\"\n",
    "        return self.scale\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns a reparameterised sample with the shape of the location parameter.\n",
    "        \"\"\"\n",
    "        eps = torch.randn_like(self.scale)\n",
    "        return self.loc + self.scale * eps\n",
    "    \n",
    "    def log_pdf(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability density of x.\n",
    "        \n",
    "        :param x: a tensor of Gaussian samples (same shape as the location parameter)\n",
    "        :returns:  tensor of log probabilities densities\n",
    "        \"\"\"\n",
    "        var = self.scale ** 2\n",
    "        return -(x - self.loc) ** 2 / 2 / var - 1./2 * np.log(2 * np.pi) - torch.log(self.scale)\n",
    "    \n",
    "    def kl(self, other: 'Normal'):\n",
    "        \"\"\"\n",
    "        The KL divergence between two Gaussians\n",
    "        :returns: a tensor of KL values with the same shape as the parameters of self.\n",
    "        \"\"\"\n",
    "        self_var = self.scale ** 2\n",
    "        other_var = other.scale ** 2\n",
    "        \n",
    "        return (torch.log(other.scale / self.scale) +\n",
    "                ((self.loc - other.loc)**2 + self_var - other_var) / 2 / other_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7N6FU3c2V86"
   },
   "outputs": [],
   "source": [
    "# tests for Normal\n",
    "torch.manual_seed(0xBADF00D)\n",
    "dummy_loc = torch.randn(10, requires_grad=True)\n",
    "dummy_scale = torch.linspace(0.1, 5.0, 10, requires_grad=True)\n",
    "dummy_samples = Normal(dummy_loc, dummy_scale).sample()\n",
    "sample_pdf = Normal(dummy_loc, dummy_scale).log_pdf(dummy_samples)\n",
    "\n",
    "dummy_grads = torch.autograd.grad(dummy_samples.mean(), [dummy_loc, dummy_scale], allow_unused=True)\n",
    "assert all(grad is not None for grad in dummy_grads), \"samples are not differentiable w.r.t. loc and/or scale. \"\\\n",
    "                                                      \" Make sure you use torch throughout the implementation \"\n",
    "assert sample_pdf.shape == dummy_loc.shape\n",
    "\n",
    "# kl divergence\n",
    "dummy_loc_2 = torch.randn(10, requires_grad=True)\n",
    "\n",
    "normal1 = Normal(dummy_loc, dummy_scale)\n",
    "normal2 = Normal(dummy_loc_2, 5.1 - dummy_scale)\n",
    "dummy_kl = normal1.kl(normal2)\n",
    "\n",
    "dummy_kl_ref = torch.tensor([3.423, 1.468, 0.8952, 0.4297, 0.1506, 0.2868, 0.8887, 4.5, 21.904, 1355.1639])\n",
    "assert dummy_kl.shape == dummy_kl_ref.shape, \"please return a batch of KL divergence with the same shape as self.loc\"\n",
    "assert torch.allclose(dummy_kl, dummy_kl_ref, rtol=1e-3, atol=1e-3), \"your KL implementation is off\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsgJqripsmDP"
   },
   "source": [
    "Then we should implement the inference model $q(z | x, \\lambda)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUqjJzdXYtMa"
   },
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedder, hidden_size,\n",
    "                 latent_size, pad_idx, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Implement the layers in the inference model.\n",
    "        \n",
    "        :param vocab_size: size of the vocabulary of the language\n",
    "        :param embedder: embedding layer\n",
    "        :param hidden_size: size of recurrent cell\n",
    "        :param latent_size: size D of the latent variable\n",
    "        :param pad_idx: id of the -PAD- token\n",
    "        :param bidirectional: whether we condition on x via a bidirectional or \n",
    "          unidirectional encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_layer = embedder\n",
    "        emb_size = self.emb_layer.embedding_dim\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True, \n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        encoding_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.latent_layer = nn.Linear(encoding_size, 2 * latent_size)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(2 * latent_size, latent_size)\n",
    "        self.sigma_layer = nn.Linear(2 * latent_size, latent_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x, seq_mask, seq_len) -> Normal:\n",
    "        \"\"\"\n",
    "        Return an inference Gaussian per instance in the mini-batch\n",
    "        :param x: sentences [B, T] as token ids\n",
    "        :param seq_mask: indicates valid positions vs padding positions [B, T]\n",
    "        :param seq_len: the length of the sequences [B]\n",
    "        :return: Gaussian approximate posterior\n",
    "        \"\"\"\n",
    "        \n",
    "        embs = self.emb_layer(x).detach()\n",
    "        packed_embs = pack_padded_sequence(embs, seq_len, batch_first=True)\n",
    "        _, (h_n, c_n) = self.lstm(packed_embs, None)\n",
    "        if self.lstm.bidirectional:\n",
    "            h_n = torch.cat([h_n[0], h_n[1]], dim=-1)\n",
    "        h = torch.tanh(self.latent_layer(h_n))\n",
    "        \n",
    "        mu = self.mu_layer(h)\n",
    "        sigma = F.softplus(self.sigma_layer(h))\n",
    "\n",
    "        return Normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dc05nZtd3s9l"
   },
   "outputs": [],
   "source": [
    "# tests for inference model\n",
    "pad_idx = vocab.word_to_idx['<pad>']\n",
    "\n",
    "dummy_inference_model = InferenceModel(\n",
    "    vocab_size=vocab.size(),\n",
    "    embedder=nn.Embedding(vocab.size(), 64, padding_idx=pad_idx),\n",
    "    hidden_size=128, latent_size=16, pad_idx=pad_idx, bidirectional=True\n",
    ").to(device=device)\n",
    "dummy_batch_size = 32\n",
    "dummy_dataloader = SortingTextDataLoader(DataLoader(train_dataset, batch_size=dummy_batch_size))\n",
    "dummy_sentences = next(dummy_dataloader)\n",
    "\n",
    "x_in, _, seq_mask, seq_len = create_batch(dummy_sentences, vocab, device)\n",
    "\n",
    "q_z_given_x = dummy_inference_model.forward(x_in, seq_mask, seq_len)\n",
    "assert isinstance(q_z_given_x, Normal), \"inference model should return a Normal distribution\"\n",
    "assert q_z_given_x.loc.shape == q_z_given_x.scale.shape, \"loc and scale must be of the same size\"\n",
    "assert q_z_given_x.loc.shape == (dummy_batch_size, 16), \"model must produce [batch_size x latent_size] units.\"\n",
    "assert torch.all(q_z_given_x.scale >= 0), \"scale can't be negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EB7rVW7ZsBHP"
   },
   "source": [
    "Then we should implement the generative model, we call it BowmanLM after one of the [authors of the model](https://arxiv.org/abs/1511.06349)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXXS-JDbpRND"
   },
   "outputs": [],
   "source": [
    "class BowmanLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, latent_size,\n",
    "                 pad_idx, dropout=0.):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the vocabulary of the language\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param hidden_size: dimensionality of recurrent cell\n",
    "        :param latent_size: this is D the dimensionality of the latent variable z\n",
    "        :param pad_idx: the id reserved to the -PAD- token\n",
    "        :param dropout: a dropout rate (you can ignore this for now)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
    "        self.bridge = nn.Linear(latent_size, hidden_size)\n",
    "        self.projection = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def init_hidden(self, z):\n",
    "        \"\"\"\n",
    "        Returns the hidden state of the LSTM initialized with a projection of a given z.\n",
    "        :param z: [B, D]\n",
    "        :returns: [B, D] hidden state, [B, D] cell state\n",
    "        \"\"\"\n",
    "        h = self.bridge(z).unsqueeze(0)\n",
    "        c = self.bridge(z).unsqueeze(0)\n",
    "        return (h, c)\n",
    "    \n",
    "    def step(self, prev_x, z, hidden):\n",
    "        \"\"\"\n",
    "        Performs a single LSTM step for a given previous word and hidden state.\n",
    "        Returns the unnormalized log probabilities (logits) over the vocabulary for this time step. \n",
    "        :param prev_x: [B] id of the previous token\n",
    "        :param z: [B, D] latent variable\n",
    "        :param hidden:  hidden ([B, H] state, [B, H] cell)\n",
    "        \"\"\"\n",
    "        x_embed = self.dropout_layer(self.embedder(prev_x))\n",
    "        output, hidden = self.lstm(x_embed, hidden)\n",
    "        scores = self.projection(self.dropout_layer(output))\n",
    "        return scores, hidden\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        \"\"\"\n",
    "        Performs an entire forward pass given a sequence of words x and a z.\n",
    "        :param x: [B, T] token ids\n",
    "        :param z: [B, D] a latent sample\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(z)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):  # through time\n",
    "            prev_x = x[:, t].unsqueeze(-1)\n",
    "            scores, hidden = self.step(prev_x, z, hidden)\n",
    "            outputs.append(scores)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "    def loss(self, scores, targets, pz, qz, free_nats=0., evaluation=False):\n",
    "        \"\"\"\n",
    "        Computes the terms in the loss (negative ELBO) given the \n",
    "            scores (unnormalized log-probabilities), targets,\n",
    "        the prior distribution p(z), and the approximate posterior distribution q(z|x).\n",
    "        \n",
    "        If free_nats is nonzero it will clamp the KL divergence between the posterior\n",
    "        and prior to that value, preventing gradient propagation via the KL if it's\n",
    "        below that value. \n",
    "        \n",
    "        If evaluation is set to true, the loss will be summed instead\n",
    "        of averaged over the batch. \n",
    "        \n",
    "        Returns the reconstruction loss and the KL. \n",
    "        \n",
    "        The loss\n",
    "        can be computed from those as loss = rec_loss - KL.\n",
    "        \n",
    "        :returns:\n",
    "            negative log likelihood (scalar), KL (scalar)\n",
    "            (use mean for training mode and sum for evaluation mode)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Approximate E[log P(x|z)].\n",
    "        scores = scores.permute(0, 2, 1)\n",
    "        reconstruction_loss = F.cross_entropy(scores, targets, \n",
    "                                              ignore_index=self.pad_idx, \n",
    "                                              reduction=\"none\")\n",
    "        reconstruction_loss = reconstruction_loss.sum(dim=1)  # sum through time\n",
    "        # Compute the KL divergence and clamp to at least the given amount of free nats\n",
    "        KL = qz.kl(pz).sum(dim=1)\n",
    "        KL = torch.clamp(KL, min=free_nats)\n",
    "        \n",
    "        # For evaluation return the sum of individual components, for\n",
    "        # training return the mean of those components.\n",
    "        if evaluation:\n",
    "            return (reconstruction_loss.sum(), KL.sum())\n",
    "        else:\n",
    "            return (reconstruction_loss.mean(), KL.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53Aztjb5ScR2"
   },
   "source": [
    "The code below is used to assess the model and also investigate what it learned. We implemented it for you, so that you can focus on the VAE part. It's useful however to learn from this example: we do interesting things like computing perplexity and sampling novel sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLfa31jEScR7"
   },
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "During training we'd like to keep track of some evaluation metrics on the validation data in order to keep track of how our model is doing and to perform early stopping. One simple metric we can compute is the ELBO on all the validation or test data using a single sample from the approximate posterior $q(z|x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URdX_JJ5ScR9"
   },
   "outputs": [],
   "source": [
    "def eval_elbo(model, inference_model, eval_dataset, vocab, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Computes a single sample estimate of the ELBO on a given dataset.\n",
    "    \"\"\"\n",
    "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    sorted_dl = SortingTextDataLoader(dl)\n",
    "    \n",
    "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
    "    model.eval()\n",
    "\n",
    "    total_rec_loss = 0.\n",
    "    total_KL = 0.\n",
    "    num_sentences = 0\n",
    "        \n",
    "    # We don't need to compute gradients for this.\n",
    "    with torch.no_grad():\n",
    "        for sentences in sorted_dl:    \n",
    "            x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device)\n",
    "            \n",
    "            # Infer the approximate posterior and construct the prior.\n",
    "            qz = inference_model(x_in, seq_mask, seq_len)\n",
    "            pz = Normal(torch.zeros_like(qz.mean()), \n",
    "                        torch.ones_like(qz.std()))\n",
    "            \n",
    "            # Compute the unnormalized probabilities using a single sample from the\n",
    "            # approximate posterior.\n",
    "            z = qz.sample()\n",
    "            scores = model(x_in, z)\n",
    "            \n",
    "            # Compute the reconstruction loss and KL divergence.\n",
    "            reconstruction_loss, KL = model.loss(scores, x_out, pz, qz,\n",
    "                                                 free_nats=0.,\n",
    "                                                 evaluation=True)\n",
    "            total_rec_loss += reconstruction_loss\n",
    "            total_KL += KL\n",
    "            num_sentences += x_in.size(0)\n",
    "\n",
    "    # Return the average reconstruction loss and KL.\n",
    "    avg_rec_loss = total_rec_loss / num_sentences\n",
    "    avg_KL = total_KL / num_sentences\n",
    "    return avg_rec_loss, avg_KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "colab_type": "code",
    "id": "dNxOoesR4Hh5",
    "outputId": "26afd358-8050-484e-da03-fcb120a4b025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(201.0963, device='cuda:2') tensor(1.7132, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# test for your BowmanLM implementation with elbo. This may take a few seconds\n",
    "dummy_lm = BowmanLM(vocab.size(), emb_size=64, hidden_size=128, \n",
    "                        latent_size=16, pad_idx=pad_idx).to(device=device)\n",
    "\n",
    "!head -n 128 {val_file} > ./dummy_dataset\n",
    "dummy_data = TextDataset('./dummy_dataset')\n",
    "dummy_rec_loss, dummy_kl = eval_elbo(dummy_lm, dummy_inference_model,\n",
    "                                     dummy_data, vocab, device)\n",
    "print(dummy_rec_loss, dummy_kl)\n",
    "assert dummy_rec_loss.item() > 0 and dummy_kl.item() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGO7RBYgScR_"
   },
   "source": [
    "\n",
    "A common metric to evaluate language models is the perplexity per word. The perplexity per word for a dataset is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ppl}(\\mathcal{D}) = \\exp\\left(-\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}} \\sum_{k=1}^{|\\mathcal{D}|} \\log P(x^{(k)})\\right) \n",
    "\\end{align}\n",
    "\n",
    "where $n^{(k)} = |x^{(k)}|$ is the number of tokens in a sentence and $P(x^{(k)})$ is the probability that our model assigns to the datapoint $x^{(k)}$. In order to compute $\\log P(x)$ for our model we need to evaluate the integral:\n",
    "\n",
    "\\begin{align}\n",
    "    P(x) = \\int P(x|z) p(z) dz\n",
    "\\end{align}\n",
    "\n",
    "As this is an integral  cannot be compute in closed-form, we have two options: we can use the earlier derived lower-bound on the log-likelihood, which will give us an upper-bound on the perplexity, or we can make an importance sampling estimate using our approximate posterior distribution. The importance sampling (IS) estimate can be done as:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat P(x) &\\overset{\\text{IS}}{\\approx} \\frac{1}{S} \\sum_{s=1}^{S} \\frac{p(z^{(s)})p(x|z^{(s)})}{q(z^{(s)}|x)} & \\text{where }z^{(s)} \\sim q(z|x)\n",
    "\\end{align}\n",
    "\n",
    "where $S$ is the number of samples.\n",
    "\n",
    "Then our perplexity becomes:\n",
    "\\begin{align}\n",
    "    &\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log p(x^{(k)}) \\\\\n",
    "    &\\approx \\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log \\frac{1}{S} \\sum_{s=1}^{S} \\frac{p(z^{(s)})p(x^{(k)}|z^{(s)})}{q(z^{(s)}|x^{(k)})} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We define the function `eval_perplexity` below that implements this importance sampling estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnaIidkwScSA"
   },
   "outputs": [],
   "source": [
    "def eval_perplexity(model, inference_model, eval_dataset, vocab, device, \n",
    "                    n_samples, batch_size=128):\n",
    "    \"\"\"\n",
    "    Estimates the per-word perplexity using importance sampling with the\n",
    "    given number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    sorted_dl = SortingTextDataLoader(dl)\n",
    "    \n",
    "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
    "    model.eval()\n",
    "    \n",
    "    log_px = 0.\n",
    "    num_predictions = 0\n",
    "    num_sentences = 0\n",
    "     \n",
    "    # We don't need to compute gradients for this.\n",
    "    with torch.no_grad():\n",
    "        for sentences in sorted_dl:\n",
    "            x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device)\n",
    "            \n",
    "            # Infer the approximate posterior and construct the prior.\n",
    "            qz = inference_model(x_in, seq_mask, seq_len)\n",
    "            pz = Normal(torch.zeros_like(qz.mean()), torch.ones_like(qz.std()))\n",
    "\n",
    "            # Create an array to hold all samples for this batch.\n",
    "            batch_size = x_in.size(0)\n",
    "            log_px_samples = torch.zeros(n_samples, batch_size)\n",
    "            \n",
    "            # Sample log P(x) n_samples times.\n",
    "            for s in range(n_samples):\n",
    "                \n",
    "                # Sample a z^s from the posterior.\n",
    "                z = qz.sample()\n",
    "                \n",
    "                # Compute log P(x^k|z^s)\n",
    "                scores = model(x_in, z)\n",
    "                cond_log_prob = F.log_softmax(scores, dim=-1)\n",
    "                cond_log_prob = torch.gather(cond_log_prob, 2, x_out.unsqueeze(-1)).squeeze() # B x T\n",
    "                cond_log_prob = (cond_log_prob * seq_mask.type_as(cond_log_prob)).sum(dim=1) # B\n",
    "                \n",
    "                # Compute log p(z^s) and log q(z^s|x^k)\n",
    "                prior_log_prob = pz.log_pdf(z).sum(dim=1) # B\n",
    "                posterior_log_prob = qz.log_pdf(z).sum(dim=1) # B\n",
    "                \n",
    "                # Store the sample for log P(x^k) importance weighted with p(z^s)/q(z^s|x^k).\n",
    "                log_px_sample = cond_log_prob + prior_log_prob - posterior_log_prob\n",
    "                log_px_samples[s] = log_px_sample\n",
    "                \n",
    "            # Average over the number of samples and count the number of predictions made this batch.\n",
    "            log_px_batch = torch.logsumexp(log_px_samples, dim=0) - \\\n",
    "                    torch.log(torch.Tensor([n_samples]))\n",
    "            log_px += log_px_batch.sum()\n",
    "            num_predictions += seq_len.sum()\n",
    "            num_sentences += seq_len.size(0)\n",
    "\n",
    "    # Compute and return the perplexity per word.\n",
    "    perplexity = torch.exp(-log_px / num_predictions)\n",
    "    NLL = -log_px / num_sentences\n",
    "    return perplexity, NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvoVmedYScSC"
   },
   "source": [
    "Lastly, we want to occasionally qualitatively see the performance of the model during training, by letting it reconstruct a given sentence from the latent space. This gives us an idea of whether the model is using the latent space to encode some semantics about the data. For this we use a deterministic greedy decoding algorithm, that chooses the word with maximum probability at every time step, and feeds that word into the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDD5XF1GScSC"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, z, vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    Greedily decodes a sentence from a given z, by picking the word with\n",
    "    maximum probability at each time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Don't compute gradients.\n",
    "    with torch.no_grad():\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # We feed the model the start-of-sentence symbol at the first time step.\n",
    "        prev_x = torch.ones(batch_size, 1, dtype=torch.long).fill_(vocab[SOS_TOKEN]).to(z.device)\n",
    "        \n",
    "        # Initialize the hidden state from z.\n",
    "        hidden = model.init_hidden(z)\n",
    "\n",
    "        predictions = []\n",
    "        for t in range(max_len):\n",
    "            scores, hidden = model.step(prev_x, z, hidden)\n",
    "            \n",
    "            # Choose the argmax of the unnnormalized probabilities as the\n",
    "            # prediction for this time step.\n",
    "            prediction = torch.argmax(scores, dim=-1)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            prev_x = prediction.view(batch_size, 1)\n",
    "            \n",
    "        return torch.cat(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GERuGgChScSE"
   },
   "source": [
    "# Training\n",
    "\n",
    "Now it's time to train the model. We use early stopping on the validation perplexity for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3386
    },
    "colab_type": "code",
    "id": "XH6ocHxaScSF",
    "outputId": "55ac32d8-3539-43cc-964d-414c74f67a24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) step 0: training ELBO (KL) = -196.57 (5.0000) -- KL weight = 0.00 -- validation ELBO (KL) = -197.16 (1.9932)\n",
      "(1) step 100: training ELBO (KL) = -136.66 (16.4289) -- KL weight = 0.01 -- validation ELBO (KL) = -121.14 (14.5677)\n",
      "(1) step 200: training ELBO (KL) = -126.89 (10.9698) -- KL weight = 0.02 -- validation ELBO (KL) = -126.08 (7.1321)\n",
      "(1) step 300: training ELBO (KL) = -130.80 (6.1360) -- KL weight = 0.03 -- validation ELBO (KL) = -124.92 (6.3052)\n",
      "(1) step 400: training ELBO (KL) = -127.19 (5.9310) -- KL weight = 0.04 -- validation ELBO (KL) = -123.18 (6.1317)\n",
      "(1) step 500: training ELBO (KL) = -127.13 (5.9715) -- KL weight = 0.04 -- validation ELBO (KL) = -121.54 (6.2612)\n",
      "Finished epoch 1\n",
      "Evaluation epoch 1:\n",
      " - validation perplexity: 1231.90\n",
      " - validation NLL: 132.21\n",
      " - validation ELBO (KL) = -121.49 (5.96)\n",
      "-- Original sentence: \"It would have been too late to think about on Friday\"\n",
      "-- Model reconstruction: \"The company 's the company 's the company 's the company 's the company 's the company 's the company 's the company of the company 's the company of the company 's the company of the company 's the company of the company 's the company of the company\"\n",
      "(2) step 600: training ELBO (KL) = -126.02 (6.0154) -- KL weight = 0.05 -- validation ELBO (KL) = -119.69 (6.6091)\n",
      "(2) step 700: training ELBO (KL) = -123.39 (6.1769) -- KL weight = 0.06 -- validation ELBO (KL) = -118.48 (6.4383)\n",
      "(2) step 800: training ELBO (KL) = -122.38 (6.4391) -- KL weight = 0.07 -- validation ELBO (KL) = -117.06 (6.6439)\n",
      "(2) step 900: training ELBO (KL) = -121.30 (6.5735) -- KL weight = 0.08 -- validation ELBO (KL) = -116.04 (6.6441)\n",
      "(2) step 1000: training ELBO (KL) = -120.28 (6.6059) -- KL weight = 0.09 -- validation ELBO (KL) = -114.87 (6.7793)\n",
      "Finished epoch 2\n",
      "Evaluation epoch 2:\n",
      " - validation perplexity: 935.54\n",
      " - validation NLL: 127.09\n",
      " - validation ELBO (KL) = -114.84 (6.59)\n",
      "-- Original sentence: \"Knight-Ridder Inc. said it would report increased earnings per share for the third quarter contrary to reported analysts ' comments that the publishing company 's earnings would be down\"\n",
      "-- Model reconstruction: \"An the company 's the company 's the company 's the company 's the company 's the company 's the company 's the company 's the company 's the company 's\"\n",
      "(3) step 1100: training ELBO (KL) = -118.78 (6.5250) -- KL weight = 0.10 -- validation ELBO (KL) = -114.58 (6.6749)\n",
      "(3) step 1200: training ELBO (KL) = -117.81 (6.4864) -- KL weight = 0.11 -- validation ELBO (KL) = -113.78 (6.3303)\n",
      "(3) step 1300: training ELBO (KL) = -117.26 (6.3385) -- KL weight = 0.11 -- validation ELBO (KL) = -112.97 (6.3614)\n",
      "(3) step 1400: training ELBO (KL) = -118.47 (6.1988) -- KL weight = 0.12 -- validation ELBO (KL) = -112.51 (6.2583)\n",
      "(3) step 1500: training ELBO (KL) = -117.51 (6.0984) -- KL weight = 0.13 -- validation ELBO (KL) = -111.88 (6.0808)\n",
      "Finished epoch 3\n",
      "Evaluation epoch 3:\n",
      " - validation perplexity: 747.01\n",
      " - validation NLL: 122.91\n",
      " - validation ELBO (KL) = -111.45 (6.05)\n",
      "-- Original sentence: \"The ad would have run during the World Series tomorrow replacing the debut commercial of Shearson 's new ad campaign Leadership by Example\"\n",
      "-- Model reconstruction: \"The company said it is n't be a new company 's the company 's the company 's the company 's the company 's the company 's\"\n",
      "(4) step 1600: training ELBO (KL) = -115.93 (5.9842) -- KL weight = 0.14 -- validation ELBO (KL) = -111.51 (5.9962)\n",
      "(4) step 1700: training ELBO (KL) = -115.59 (5.9960) -- KL weight = 0.15 -- validation ELBO (KL) = -110.79 (6.1040)\n",
      "(4) step 1800: training ELBO (KL) = -115.61 (5.9593) -- KL weight = 0.16 -- validation ELBO (KL) = -111.18 (5.7892)\n",
      "(4) step 1900: training ELBO (KL) = -115.40 (5.9117) -- KL weight = 0.17 -- validation ELBO (KL) = -110.11 (5.8473)\n",
      "(4) step 2000: training ELBO (KL) = -114.90 (5.8901) -- KL weight = 0.18 -- validation ELBO (KL) = -109.82 (5.8042)\n",
      "(4) step 2100: training ELBO (KL) = -114.29 (5.8579) -- KL weight = 0.18 -- validation ELBO (KL) = -109.39 (5.8036)\n",
      "Finished epoch 4\n",
      "Evaluation epoch 4:\n",
      " - validation perplexity: 652.11\n",
      " - validation NLL: 120.39\n",
      " - validation ELBO (KL) = -109.56 (5.67)\n",
      "-- Original sentence: \"The grand marshal of this parade would appear to have been excess leverage\"\n",
      "-- Model reconstruction: \"The company said it is n't be a lot of the company 's\"\n",
      "(5) step 2200: training ELBO (KL) = -113.26 (5.8893) -- KL weight = 0.19 -- validation ELBO (KL) = -109.04 (5.9846)\n",
      "(5) step 2300: training ELBO (KL) = -113.20 (5.8068) -- KL weight = 0.20 -- validation ELBO (KL) = -108.73 (5.7380)\n",
      "(5) step 2400: training ELBO (KL) = -112.39 (5.7392) -- KL weight = 0.21 -- validation ELBO (KL) = -108.36 (5.7760)\n",
      "(5) step 2500: training ELBO (KL) = -112.54 (5.7592) -- KL weight = 0.22 -- validation ELBO (KL) = -108.17 (5.7564)\n",
      "(5) step 2600: training ELBO (KL) = -112.84 (5.7284) -- KL weight = 0.23 -- validation ELBO (KL) = -107.49 (5.8131)\n",
      "Finished epoch 5\n",
      "Evaluation epoch 5:\n",
      " - validation perplexity: 589.03\n",
      " - validation NLL: 118.50\n",
      " - validation ELBO (KL) = -107.68 (5.61)\n",
      "-- Original sentence: \"That 's not an average to soothe Giant rooters\"\n",
      "-- Model reconstruction: \"And the company 's a few years ago\"\n",
      "(6) step 2700: training ELBO (KL) = -111.42 (5.7587) -- KL weight = 0.24 -- validation ELBO (KL) = -107.34 (5.7346)\n",
      "(6) step 2800: training ELBO (KL) = -111.29 (5.7362) -- KL weight = 0.25 -- validation ELBO (KL) = -107.38 (5.6506)\n",
      "(6) step 2900: training ELBO (KL) = -110.98 (5.6905) -- KL weight = 0.25 -- validation ELBO (KL) = -106.83 (5.6567)\n",
      "(6) step 3000: training ELBO (KL) = -109.81 (5.6735) -- KL weight = 0.26 -- validation ELBO (KL) = -106.84 (5.7095)\n",
      "(6) step 3100: training ELBO (KL) = -111.10 (5.6404) -- KL weight = 0.27 -- validation ELBO (KL) = -106.48 (5.6814)\n",
      "Finished epoch 6\n",
      "Evaluation epoch 6:\n",
      " - validation perplexity: 555.28\n",
      " - validation NLL: 117.40\n",
      " - validation ELBO (KL) = -106.28 (5.75)\n",
      "-- Original sentence: \"The Dow Jones Industrial Average opened down 1.64 shortly after 9:30\"\n",
      "-- Model reconstruction: \"The company said it was n't disclosed to be a share\"\n",
      "(7) step 3200: training ELBO (KL) = -110.54 (5.6660) -- KL weight = 0.28 -- validation ELBO (KL) = -106.36 (5.6868)\n",
      "(7) step 3300: training ELBO (KL) = -109.72 (5.6948) -- KL weight = 0.29 -- validation ELBO (KL) = -106.22 (5.6696)\n",
      "(7) step 3400: training ELBO (KL) = -110.37 (5.6259) -- KL weight = 0.30 -- validation ELBO (KL) = -105.79 (5.6611)\n",
      "(7) step 3500: training ELBO (KL) = -109.15 (5.6420) -- KL weight = 0.31 -- validation ELBO (KL) = -105.78 (5.6402)\n",
      "(7) step 3600: training ELBO (KL) = -108.69 (5.6348) -- KL weight = 0.32 -- validation ELBO (KL) = -105.49 (5.6378)\n",
      "Finished epoch 7\n",
      "Evaluation epoch 7:\n",
      " - validation perplexity: 525.60\n",
      " - validation NLL: 116.38\n",
      " - validation ELBO (KL) = -105.56 (5.60)\n",
      "-- Original sentence: \"PRIME RATE 10 1\\/2 %\"\n",
      "-- Model reconstruction: \"Revenue rose to yield %\"\n",
      "(8) step 3700: training ELBO (KL) = -108.08 (5.6191) -- KL weight = 0.32 -- validation ELBO (KL) = -105.27 (5.6456)\n",
      "(8) step 3800: training ELBO (KL) = -108.00 (5.5995) -- KL weight = 0.33 -- validation ELBO (KL) = -105.29 (5.6986)\n",
      "(8) step 3900: training ELBO (KL) = -107.48 (5.5751) -- KL weight = 0.34 -- validation ELBO (KL) = -105.21 (5.5760)\n",
      "(8) step 4000: training ELBO (KL) = -107.80 (5.5782) -- KL weight = 0.35 -- validation ELBO (KL) = -105.80 (5.5009)\n",
      "(8) step 4100: training ELBO (KL) = -108.03 (5.5604) -- KL weight = 0.36 -- validation ELBO (KL) = -104.72 (5.6314)\n",
      "(8) step 4200: training ELBO (KL) = -108.55 (5.5215) -- KL weight = 0.37 -- validation ELBO (KL) = -104.68 (5.6083)\n",
      "Finished epoch 8\n",
      "Evaluation epoch 8:\n",
      " - validation perplexity: 499.17\n",
      " - validation NLL: 115.42\n",
      " - validation ELBO (KL) = -104.62 (5.58)\n",
      "-- Original sentence: \"A. manual typewriters B. black-and-white snapshots C. radio adventure shows\"\n",
      "-- Model reconstruction: \"FEDERAL FUNDS Inc. said it was a share in the third quarter\"\n",
      "(9) step 4300: training ELBO (KL) = -106.94 (5.5230) -- KL weight = 0.38 -- validation ELBO (KL) = -104.89 (5.4942)\n",
      "(9) step 4400: training ELBO (KL) = -106.40 (5.5348) -- KL weight = 0.39 -- validation ELBO (KL) = -104.59 (5.5764)\n",
      "(9) step 4500: training ELBO (KL) = -107.09 (5.5255) -- KL weight = 0.39 -- validation ELBO (KL) = -104.43 (5.5212)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9) step 4600: training ELBO (KL) = -106.88 (5.5157) -- KL weight = 0.40 -- validation ELBO (KL) = -104.10 (5.5650)\n",
      "(9) step 4700: training ELBO (KL) = -106.35 (5.5267) -- KL weight = 0.41 -- validation ELBO (KL) = -104.01 (5.5454)\n",
      "Finished epoch 9\n",
      "Evaluation epoch 9:\n",
      " - validation perplexity: 479.05\n",
      " - validation NLL: 114.66\n",
      " - validation ELBO (KL) = -104.02 (5.50)\n",
      "-- Original sentence: \"In a sharply weaker London market yesterday Waterford shares were down 15 pence at 50 pence 79 cents\"\n",
      "-- Model reconstruction: \"In the past time the company 's largest company said it was a loss of the company 's\"\n",
      "(10) step 4800: training ELBO (KL) = -106.14 (5.5081) -- KL weight = 0.42 -- validation ELBO (KL) = -104.12 (5.5555)\n",
      "(10) step 4900: training ELBO (KL) = -104.62 (5.5019) -- KL weight = 0.43 -- validation ELBO (KL) = -104.09 (5.5639)\n",
      "(10) step 5000: training ELBO (KL) = -105.84 (5.5069) -- KL weight = 0.44 -- validation ELBO (KL) = -103.94 (5.4810)\n",
      "(10) step 5100: training ELBO (KL) = -105.30 (5.4956) -- KL weight = 0.45 -- validation ELBO (KL) = -103.75 (5.5530)\n",
      "(10) step 5200: training ELBO (KL) = -107.33 (5.4820) -- KL weight = 0.46 -- validation ELBO (KL) = -103.68 (5.5193)\n",
      "Finished epoch 10\n",
      "Evaluation epoch 10:\n",
      " - validation perplexity: 468.14\n",
      " - validation NLL: 114.23\n",
      " - validation ELBO (KL) = -103.66 (5.47)\n",
      "-- Original sentence: \"In the opening game besides Steinbach and Stewart there was Walt Weiss a twiggy-looking second-year shortstop who had lost a couple months of the season to knee surgery\"\n",
      "-- Model reconstruction: \"In the past time the company 's largest company said it was a major company 's largest company 's largest stock market and the company 's largest\"\n",
      "(11) step 5300: training ELBO (KL) = -104.43 (5.4908) -- KL weight = 0.46 -- validation ELBO (KL) = -103.66 (5.5136)\n",
      "(11) step 5400: training ELBO (KL) = -104.96 (5.4679) -- KL weight = 0.47 -- validation ELBO (KL) = -103.76 (5.4598)\n",
      "(11) step 5500: training ELBO (KL) = -104.50 (5.4550) -- KL weight = 0.48 -- validation ELBO (KL) = -103.70 (5.4821)\n",
      "(11) step 5600: training ELBO (KL) = -104.80 (5.4527) -- KL weight = 0.49 -- validation ELBO (KL) = -103.55 (5.3750)\n",
      "(11) step 5700: training ELBO (KL) = -105.54 (5.4382) -- KL weight = 0.50 -- validation ELBO (KL) = -103.50 (5.4322)\n",
      "Finished epoch 11\n",
      "Evaluation epoch 11:\n",
      " - validation perplexity: 461.14\n",
      " - validation NLL: 113.95\n",
      " - validation ELBO (KL) = -103.45 (5.46)\n",
      "-- Original sentence: \"Pittsburg Kan.-based National Pizza said the transaction will be financed under revolving credit agreement\"\n",
      "-- Model reconstruction: \"The National Mortgage Corp. said it will be a new company for the company\"\n",
      "(12) step 5800: training ELBO (KL) = -104.15 (5.4143) -- KL weight = 0.51 -- validation ELBO (KL) = -103.89 (5.3932)\n",
      "(12) step 5900: training ELBO (KL) = -102.49 (5.4080) -- KL weight = 0.52 -- validation ELBO (KL) = -103.54 (5.3642)\n",
      "(12) step 6000: training ELBO (KL) = -103.96 (5.4084) -- KL weight = 0.53 -- validation ELBO (KL) = -103.32 (5.4327)\n",
      "(12) step 6100: training ELBO (KL) = -104.05 (5.3996) -- KL weight = 0.54 -- validation ELBO (KL) = -103.39 (5.4256)\n",
      "(12) step 6200: training ELBO (KL) = -104.25 (5.3767) -- KL weight = 0.54 -- validation ELBO (KL) = -103.22 (5.4198)\n",
      "(12) step 6300: training ELBO (KL) = -104.36 (5.3758) -- KL weight = 0.55 -- validation ELBO (KL) = -103.29 (5.4540)\n",
      "Finished epoch 12\n",
      "Evaluation epoch 12:\n",
      " - validation perplexity: 456.87\n",
      " - validation NLL: 113.78\n",
      " - validation ELBO (KL) = -103.43 (5.39)\n",
      "-- Original sentence: \"Analysts estimate those gains at 12 % to 13 % a good part of it coming from large orders placed by a few of NCR 's major customers\"\n",
      "-- Model reconstruction: \"Analysts say the company 's largest business is n't to be able to be able to be able to be able to be able to be a good\"\n",
      "(13) step 6400: training ELBO (KL) = -102.58 (5.3735) -- KL weight = 0.56 -- validation ELBO (KL) = -103.48 (5.3791)\n",
      "(13) step 6500: training ELBO (KL) = -103.40 (5.3462) -- KL weight = 0.57 -- validation ELBO (KL) = -103.52 (5.3408)\n",
      "(13) step 6600: training ELBO (KL) = -103.10 (5.3406) -- KL weight = 0.58 -- validation ELBO (KL) = -103.43 (5.3745)\n",
      "(13) step 6700: training ELBO (KL) = -102.61 (5.3439) -- KL weight = 0.59 -- validation ELBO (KL) = -103.12 (5.3788)\n",
      "(13) step 6800: training ELBO (KL) = -103.92 (5.3332) -- KL weight = 0.60 -- validation ELBO (KL) = -103.12 (5.3419)\n",
      "Finished epoch 13\n",
      "Evaluation epoch 13:\n",
      " - validation perplexity: 446.19\n",
      " - validation NLL: 113.34\n",
      " - validation ELBO (KL) = -103.13 (5.31)\n",
      "-- Original sentence: \"These are all market excesses putting aside the artificial boosts that the tax code gives to debt over equity and what we 've seen is the market reining them in\"\n",
      "-- Model reconstruction: \"When Mr. Peters 's departure is n't a lot of the U.S. and the U.S. and the U.S. and the U.S. and the U.S. and the U.S. and other things\"\n",
      "(14) step 6900: training ELBO (KL) = -103.97 (5.3191) -- KL weight = 0.61 -- validation ELBO (KL) = -103.14 (5.3235)\n",
      "(14) step 7000: training ELBO (KL) = -102.49 (5.3063) -- KL weight = 0.61 -- validation ELBO (KL) = -103.40 (5.3018)\n",
      "(14) step 7100: training ELBO (KL) = -101.67 (5.2927) -- KL weight = 0.62 -- validation ELBO (KL) = -103.39 (5.2390)\n",
      "(14) step 7200: training ELBO (KL) = -102.49 (5.2899) -- KL weight = 0.63 -- validation ELBO (KL) = -103.21 (5.3111)\n",
      "(14) step 7300: training ELBO (KL) = -102.57 (5.2792) -- KL weight = 0.64 -- validation ELBO (KL) = -103.15 (5.3098)\n",
      "Finished epoch 14\n",
      "Evaluation epoch 14:\n",
      " - validation perplexity: 444.09\n",
      " - validation NLL: 113.25\n",
      " - validation ELBO (KL) = -103.01 (5.28)\n",
      "-- Original sentence: \"A spokesman said the guerrillas would present a cease-fire proposal during the negotiations in Costa Rica that includes constitutional and economic changes\"\n",
      "-- Model reconstruction: \"A spokesman said it expects to be a new dividend of the company 's largest stock market and the company said\"\n",
      "(15) step 7400: training ELBO (KL) = -101.58 (5.2577) -- KL weight = 0.65 -- validation ELBO (KL) = -103.26 (5.2754)\n",
      "(15) step 7500: training ELBO (KL) = -101.37 (5.2637) -- KL weight = 0.66 -- validation ELBO (KL) = -103.40 (5.2781)\n",
      "(15) step 7600: training ELBO (KL) = -102.16 (5.2673) -- KL weight = 0.67 -- validation ELBO (KL) = -103.20 (5.2457)\n",
      "(15) step 7700: training ELBO (KL) = -101.89 (5.2603) -- KL weight = 0.68 -- validation ELBO (KL) = -103.27 (5.3335)\n",
      "(15) step 7800: training ELBO (KL) = -101.54 (5.2389) -- KL weight = 0.68 -- validation ELBO (KL) = -103.10 (5.2462)\n",
      "Finished epoch 15\n",
      "Evaluation epoch 15:\n",
      " - validation perplexity: 440.94\n",
      " - validation NLL: 113.12\n",
      " - validation ELBO (KL) = -103.04 (5.24)\n",
      "-- Original sentence: \"Mr. Bandow is a Cato Institute fellow\"\n",
      "-- Model reconstruction: \"Mr. Noriega 's departure is n't disclosed\"\n",
      "(16) step 7900: training ELBO (KL) = -101.73 (5.2375) -- KL weight = 0.69 -- validation ELBO (KL) = -103.16 (5.2506)\n",
      "(16) step 8000: training ELBO (KL) = -100.49 (5.2342) -- KL weight = 0.70 -- validation ELBO (KL) = -103.17 (5.2622)\n",
      "(16) step 8100: training ELBO (KL) = -101.10 (5.2242) -- KL weight = 0.71 -- validation ELBO (KL) = -103.09 (5.2325)\n",
      "(16) step 8200: training ELBO (KL) = -101.38 (5.2218) -- KL weight = 0.72 -- validation ELBO (KL) = -103.05 (5.2833)\n",
      "(16) step 8300: training ELBO (KL) = -101.01 (5.2029) -- KL weight = 0.73 -- validation ELBO (KL) = -103.27 (5.1954)\n",
      "(16) step 8400: training ELBO (KL) = -101.75 (5.1953) -- KL weight = 0.74 -- validation ELBO (KL) = -103.12 (5.1987)\n",
      "Finished epoch 16\n",
      "Evaluation epoch 16:\n",
      " - validation perplexity: 440.59\n",
      " - validation NLL: 113.10\n",
      " - validation ELBO (KL) = -103.16 (5.15)\n",
      "-- Original sentence: \"Under the agreement Intel will invest 3 million to acquire a 4 % stake in Alliant a maker of minisupercomputers for scientists and engineers\"\n",
      "-- Model reconstruction: \"As a result of the company 's largest stock market was priced at par to yield from 1.8470 billion yen from 1.17 billion yen\"\n",
      "(17) step 8500: training ELBO (KL) = -99.08 (5.1950) -- KL weight = 0.75 -- validation ELBO (KL) = -103.19 (5.1804)\n",
      "(17) step 8600: training ELBO (KL) = -100.39 (5.1861) -- KL weight = 0.75 -- validation ELBO (KL) = -103.29 (5.2017)\n",
      "(17) step 8700: training ELBO (KL) = -101.56 (5.1723) -- KL weight = 0.76 -- validation ELBO (KL) = -103.35 (5.1614)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17) step 8800: training ELBO (KL) = -101.23 (5.1701) -- KL weight = 0.77 -- validation ELBO (KL) = -103.55 (5.0782)\n",
      "(17) step 8900: training ELBO (KL) = -100.52 (5.1635) -- KL weight = 0.78 -- validation ELBO (KL) = -103.18 (5.1881)\n",
      "Finished epoch 17\n",
      "Evaluation epoch 17:\n",
      " - validation perplexity: 444.19\n",
      " - validation NLL: 113.26\n",
      " - validation ELBO (KL) = -103.30 (5.19)\n",
      "-- Original sentence: \"While the advance cheered investors who feared a 1987-style crash would occur yesterday it was strictly a big-stock rally fed by huge buying by bargain-hunting institutions and program traders\"\n",
      "-- Model reconstruction: \"As a result of the U.S. and the U.S. and the company 's largest company said it will be a new company for the company 's largest company\"\n",
      "(18) step 9000: training ELBO (KL) = -100.86 (5.1759) -- KL weight = 0.79 -- validation ELBO (KL) = -103.65 (5.1377)\n",
      "(18) step 9100: training ELBO (KL) = -98.40 (5.1572) -- KL weight = 0.80 -- validation ELBO (KL) = -103.44 (5.1543)\n",
      "(18) step 9200: training ELBO (KL) = -100.27 (5.1538) -- KL weight = 0.81 -- validation ELBO (KL) = -103.42 (5.1581)\n",
      "(18) step 9300: training ELBO (KL) = -101.16 (5.1426) -- KL weight = 0.82 -- validation ELBO (KL) = -103.46 (5.1136)\n",
      "(18) step 9400: training ELBO (KL) = -100.53 (5.1329) -- KL weight = 0.82 -- validation ELBO (KL) = -103.32 (5.1956)\n",
      "Finished epoch 18\n",
      "Evaluation epoch 18:\n",
      " - validation perplexity: 442.39\n",
      " - validation NLL: 113.18\n",
      " - validation ELBO (KL) = -103.26 (5.15)\n",
      "-- Original sentence: \"Despite traders ' complaints Mr. Phelan said the links with the Chicago futures market worked as planned in Friday 's rout to provide a cooling-off period\"\n",
      "-- Model reconstruction: \"With the U.S. and the company 's largest business and the company 's largest business and the company 's largest business and the company 's largest business\"\n",
      "(19) step 9500: training ELBO (KL) = -99.70 (5.1422) -- KL weight = 0.83 -- validation ELBO (KL) = -103.63 (5.1494)\n",
      "(19) step 9600: training ELBO (KL) = -99.99 (5.1400) -- KL weight = 0.84 -- validation ELBO (KL) = -103.32 (5.1154)\n",
      "(19) step 9700: training ELBO (KL) = -99.19 (5.1313) -- KL weight = 0.85 -- validation ELBO (KL) = -103.53 (5.1254)\n",
      "(19) step 9800: training ELBO (KL) = -99.53 (5.1308) -- KL weight = 0.86 -- validation ELBO (KL) = -103.50 (5.1286)\n",
      "(19) step 9900: training ELBO (KL) = -99.24 (5.1275) -- KL weight = 0.87 -- validation ELBO (KL) = -103.53 (5.0998)\n",
      "Finished epoch 19\n",
      "Evaluation epoch 19:\n",
      " - validation perplexity: 447.50\n",
      " - validation NLL: 113.39\n",
      " - validation ELBO (KL) = -103.54 (5.11)\n",
      "-- Original sentence: \"Share prices plummeted across Europe yesterday in response to Friday 's New York sell-off but some issues staged a late comeback after Wall Street opened without another rout\"\n",
      "-- Model reconstruction: \"Rated single-A-2 by Moody 's Investors Service Inc. said it will be a new company for the company 's largest company 's largest company\"\n",
      "(20) step 10000: training ELBO (KL) = -99.96 (5.1253) -- KL weight = 0.88 -- validation ELBO (KL) = -103.73 (5.1462)\n",
      "(20) step 10100: training ELBO (KL) = -99.51 (5.1203) -- KL weight = 0.89 -- validation ELBO (KL) = -103.64 (5.1208)\n",
      "(20) step 10200: training ELBO (KL) = -98.39 (5.1203) -- KL weight = 0.89 -- validation ELBO (KL) = -103.85 (5.0669)\n",
      "(20) step 10300: training ELBO (KL) = -98.89 (5.1132) -- KL weight = 0.90 -- validation ELBO (KL) = -103.52 (5.1204)\n",
      "(20) step 10400: training ELBO (KL) = -99.63 (5.1134) -- KL weight = 0.91 -- validation ELBO (KL) = -103.53 (5.1119)\n",
      "(20) step 10500: training ELBO (KL) = -98.78 (5.1125) -- KL weight = 0.92 -- validation ELBO (KL) = -103.64 (5.0921)\n",
      "Finished epoch 20\n",
      "Evaluation epoch 20:\n",
      " - validation perplexity: 449.31\n",
      " - validation NLL: 113.47\n",
      " - validation ELBO (KL) = -103.59 (5.08)\n",
      "-- Original sentence: \"The decline in the German Stock Index of 203.56 points or 12.8 % to 1385.72 was the Frankfurt market 's steepest fall ever\"\n",
      "-- Model reconstruction: \"The company said it will be a new dividend of the company 's largest stock market and the company 's largest business\"\n",
      "(21) step 10600: training ELBO (KL) = -97.44 (5.1078) -- KL weight = 0.93 -- validation ELBO (KL) = -103.72 (5.1133)\n",
      "(21) step 10700: training ELBO (KL) = -97.16 (5.1076) -- KL weight = 0.94 -- validation ELBO (KL) = -103.73 (5.0877)\n",
      "(21) step 10800: training ELBO (KL) = -99.41 (5.1017) -- KL weight = 0.95 -- validation ELBO (KL) = -103.70 (5.0929)\n",
      "(21) step 10900: training ELBO (KL) = -99.22 (5.0973) -- KL weight = 0.96 -- validation ELBO (KL) = -103.55 (5.1403)\n",
      "(21) step 11000: training ELBO (KL) = -99.67 (5.0927) -- KL weight = 0.96 -- validation ELBO (KL) = -103.90 (5.0236)\n",
      "Finished epoch 21\n",
      "Evaluation epoch 21:\n",
      " - validation perplexity: 452.98\n",
      " - validation NLL: 113.62\n",
      " - validation ELBO (KL) = -103.86 (5.08)\n",
      "-- Original sentence: \"The MMI has gone better shouted one trader at about 3:15 London time as the U.S. Major Markets Index contract suddenly indicated a turnabout\"\n",
      "-- Model reconstruction: \"The company said it expects to be a new dividend of the company 's largest debt and the company 's largest business\"\n",
      "(22) step 11100: training ELBO (KL) = -98.79 (5.0868) -- KL weight = 0.97 -- validation ELBO (KL) = -103.88 (5.0728)\n",
      "(22) step 11200: training ELBO (KL) = -97.13 (5.0921) -- KL weight = 0.98 -- validation ELBO (KL) = -103.90 (5.0911)\n",
      "(22) step 11300: training ELBO (KL) = -98.22 (5.0913) -- KL weight = 0.99 -- validation ELBO (KL) = -103.97 (5.1004)\n",
      "(22) step 11400: training ELBO (KL) = -98.60 (5.0877) -- KL weight = 1.00 -- validation ELBO (KL) = -103.85 (5.0652)\n",
      "(22) step 11500: training ELBO (KL) = -98.14 (5.0873) -- KL weight = 1.00 -- validation ELBO (KL) = -104.06 (5.0472)\n",
      "Finished epoch 22\n",
      "Evaluation epoch 22:\n",
      " - validation perplexity: 454.69\n",
      " - validation NLL: 113.69\n",
      " - validation ELBO (KL) = -103.92 (5.07)\n",
      "-- Original sentence: \"Sales climbed to an estimated 245 million in fiscal 1989 ended Aug. 31 from 99.9 million in fiscal 1985\"\n",
      "-- Model reconstruction: \"Sales rose to 449 million from 1.69 million or 69 cents a share a year earlier\"\n",
      "(23) step 11600: training ELBO (KL) = -97.83 (5.0817) -- KL weight = 1.00 -- validation ELBO (KL) = -104.31 (5.0799)\n",
      "(23) step 11700: training ELBO (KL) = -97.85 (5.0884) -- KL weight = 1.00 -- validation ELBO (KL) = -104.03 (5.0836)\n",
      "(23) step 11800: training ELBO (KL) = -96.96 (5.0800) -- KL weight = 1.00 -- validation ELBO (KL) = -104.12 (5.0516)\n",
      "(23) step 11900: training ELBO (KL) = -97.79 (5.0892) -- KL weight = 1.00 -- validation ELBO (KL) = -104.27 (5.0557)\n",
      "(23) step 12000: training ELBO (KL) = -98.18 (5.0875) -- KL weight = 1.00 -- validation ELBO (KL) = -104.10 (5.0724)\n",
      "Finished epoch 23\n",
      "Evaluation epoch 23:\n",
      " - validation perplexity: 458.89\n",
      " - validation NLL: 113.86\n",
      " - validation ELBO (KL) = -104.18 (5.06)\n",
      "-- Original sentence: \"Now the firms must try hardest to prove that advertising can work this time around\"\n",
      "-- Model reconstruction: \"As a result of the U.S. and the company 's largest business is n't disclosed\"\n",
      "(24) step 12100: training ELBO (KL) = -97.59 (5.0805) -- KL weight = 1.00 -- validation ELBO (KL) = -104.00 (5.0789)\n",
      "(24) step 12200: training ELBO (KL) = -96.58 (5.0834) -- KL weight = 1.00 -- validation ELBO (KL) = -104.30 (5.0672)\n",
      "(24) step 12300: training ELBO (KL) = -97.51 (5.0809) -- KL weight = 1.00 -- validation ELBO (KL) = -104.74 (5.0656)\n",
      "(24) step 12400: training ELBO (KL) = -96.84 (5.0856) -- KL weight = 1.00 -- validation ELBO (KL) = -104.16 (5.0908)\n",
      "(24) step 12500: training ELBO (KL) = -97.66 (5.0871) -- KL weight = 1.00 -- validation ELBO (KL) = -104.28 (5.0645)\n",
      "(24) step 12600: training ELBO (KL) = -97.88 (5.0822) -- KL weight = 1.00 -- validation ELBO (KL) = -104.21 (5.0656)\n",
      "Finished epoch 24\n",
      "Evaluation epoch 24:\n",
      " - validation perplexity: 461.60\n",
      " - validation NLL: 113.97\n",
      " - validation ELBO (KL) = -104.30 (5.05)\n",
      "-- Original sentence: \"What 's more such short-term cataclysms are survivable and are no cause for panic selling\"\n",
      "-- Model reconstruction: \"To make the same time the company 's decision to be a lot of the company\"\n",
      "(25) step 12700: training ELBO (KL) = -95.61 (5.0813) -- KL weight = 1.00 -- validation ELBO (KL) = -104.27 (5.0844)\n",
      "(25) step 12800: training ELBO (KL) = -97.06 (5.0856) -- KL weight = 1.00 -- validation ELBO (KL) = -104.58 (5.0501)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25) step 12900: training ELBO (KL) = -97.03 (5.0793) -- KL weight = 1.00 -- validation ELBO (KL) = -104.41 (5.0729)\n",
      "(25) step 13000: training ELBO (KL) = -96.12 (5.0797) -- KL weight = 1.00 -- validation ELBO (KL) = -104.39 (5.0751)\n",
      "(25) step 13100: training ELBO (KL) = -97.43 (5.0798) -- KL weight = 1.00 -- validation ELBO (KL) = -104.36 (5.1113)\n",
      "Finished epoch 25\n",
      "Evaluation epoch 25:\n",
      " - validation perplexity: 466.38\n",
      " - validation NLL: 114.16\n",
      " - validation ELBO (KL) = -104.44 (5.04)\n",
      "-- Original sentence: \"The magazine will distribute 10 % of the gross revenues from the supplement as grants to innovative teachers\"\n",
      "-- Model reconstruction: \"The company said it will be a major company for the company 's largest business and other businesses\"\n"
     ]
    }
   ],
   "source": [
    "# Define the model hyperparameters.\n",
    "emb_size = 256\n",
    "hidden_size = 256\n",
    "latent_size = 16\n",
    "bidirectional_encoder = True\n",
    "free_nats = 5.\n",
    "annealing_steps = 11400\n",
    "dropout = 0.6\n",
    "word_dropout = 0.15\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "n_importance_samples = 3  # 50\n",
    "\n",
    "# Create the training data loader.\n",
    "dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "sorted_dl = SortingTextDataLoader(dl)\n",
    "\n",
    "# Create the generative model.\n",
    "model = BowmanLM(vocab_size=vocab.size(), \n",
    "                 emb_size=emb_size, \n",
    "                 hidden_size=hidden_size, \n",
    "                 latent_size=latent_size, \n",
    "                 pad_idx=vocab[PAD_TOKEN],\n",
    "                 dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create the inference model.\n",
    "inference_model = InferenceModel(vocab_size=vocab.size(),\n",
    "                                 embedder=model.embedder,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 latent_size=latent_size,\n",
    "                                 pad_idx=vocab[PAD_TOKEN],\n",
    "                                 bidirectional=bidirectional_encoder)\n",
    "inference_model = inference_model.to(device)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(inference_model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Save the best model (early stopping).\n",
    "best_model = \"./best_model.pt\"\n",
    "best_val_ppl = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "# Keep track of some statistics to plot later.\n",
    "train_ELBOs = []\n",
    "train_KLs = []\n",
    "val_ELBOs = []\n",
    "val_KLs = []\n",
    "val_perplexities = []\n",
    "val_NLLs = []\n",
    "\n",
    "step = 0\n",
    "training_ELBO = 0.\n",
    "training_KL = 0.\n",
    "num_batches = 0\n",
    "for epoch_num in range(1, num_epochs+1):\n",
    "    for sentences in sorted_dl:\n",
    "        # Make sure the model is in training mode (for dropout).\n",
    "        model.train()\n",
    "\n",
    "        # Transform the sentences to input, output, seq_len, seq_mask batches.\n",
    "        x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device,\n",
    "                                                      word_dropout=word_dropout)\n",
    "\n",
    "        # Compute the multiplier for the KL term if we do annealing.\n",
    "        if annealing_steps > 0:\n",
    "            KL_weight = min(1., (1.0 / annealing_steps) * step)\n",
    "        else:\n",
    "            KL_weight = 1.\n",
    "        \n",
    "        # Do a forward pass through the model and compute the training loss. We use\n",
    "        # a reparameterized sample from the approximate posterior during training.\n",
    "        qz = inference_model(x_in, seq_mask, seq_len)\n",
    "        pz = Normal(torch.zeros_like(qz.mean()),\n",
    "                    torch.ones_like(qz.std()))\n",
    "        \n",
    "        z = qz.sample()\n",
    "        scores = model(x_in, z)\n",
    "        rec_loss, KL = model.loss(scores, x_out, pz, qz, free_nats=free_nats)\n",
    "        loss = rec_loss + KL_weight * KL\n",
    "\n",
    "        # Backpropagate and update the model weights.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update some statistics to track for the training loss.\n",
    "        training_ELBO += -(rec_loss - KL)\n",
    "        training_KL += KL\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Every 100 steps we evaluate the model and report progress.\n",
    "        if step % 100 == 0:\n",
    "            val_rec_loss, val_KL = eval_elbo(model, inference_model, val_dataset, vocab, device)\n",
    "            val_ELBO = -(val_rec_loss - val_KL)\n",
    "            print(\"(%d) step %d: training ELBO (KL) = %.2f (%.4f) --\"\n",
    "                  \" KL weight = %.2f --\"\n",
    "                  \" validation ELBO (KL) = %.2f (%.4f)\" % \n",
    "                  (epoch_num, step, training_ELBO/num_batches, \n",
    "                   training_KL/num_batches, KL_weight, val_ELBO, val_KL))\n",
    "            \n",
    "            # Update some statistics for plotting later.\n",
    "            train_ELBOs.append((step, (training_ELBO/num_batches).item()))\n",
    "            train_KLs.append((step, (training_KL/num_batches).item()))\n",
    "            val_ELBOs.append((step, val_ELBO.item()))\n",
    "            val_KLs.append((step, val_KL.item()))\n",
    "            \n",
    "            # Reset the training statistics.\n",
    "            training_ELBO = 0.\n",
    "            training_KL = 0.\n",
    "            num_batches = 0\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "    # After an epoch we'll compute validation perplexity and save the model\n",
    "    # for early stopping if it's better than previous models.\n",
    "    print(\"Finished epoch %d\" % (epoch_num))\n",
    "    val_perplexity, val_NLL = eval_perplexity(model, inference_model, val_dataset, vocab, device, \n",
    "                                              n_importance_samples)\n",
    "    val_rec_loss, val_KL = eval_elbo(model, inference_model, val_dataset, vocab, device)\n",
    "    val_ELBO = -(val_rec_loss - val_KL)\n",
    "    \n",
    "    # Keep track of the validation perplexities / NLL.\n",
    "    val_perplexities.append((epoch_num, val_perplexity.item()))\n",
    "    val_NLLs.append((epoch_num, val_NLL.item()))\n",
    "    \n",
    "    # If validation perplexity is better, store this model for early stopping.\n",
    "    if val_perplexity < best_val_ppl:\n",
    "        best_val_ppl = val_perplexity\n",
    "        best_epoch = epoch_num\n",
    "        torch.save(model.state_dict(), best_model)\n",
    "        \n",
    "    # Print epoch statistics.\n",
    "    print(\"Evaluation epoch %d:\\n\"\n",
    "          \" - validation perplexity: %.2f\\n\"\n",
    "          \" - validation NLL: %.2f\\n\"\n",
    "          \" - validation ELBO (KL) = %.2f (%.2f)\"\n",
    "          % (epoch_num, val_perplexity, val_NLL, val_ELBO, val_KL))\n",
    "\n",
    "    # Also show some qualitative results by reconstructing a sentence from the\n",
    "    # validation data. Use the mean of the approximate posterior and greedy\n",
    "    # decoding.\n",
    "    random_sentence = val_dataset[np.random.choice(len(val_dataset))]\n",
    "    x_in, _, seq_mask, seq_len = create_batch([random_sentence], vocab, device)\n",
    "    qz = inference_model(x_in, seq_mask, seq_len)\n",
    "    z = qz.mean()\n",
    "    reconstruction = greedy_decode(model, z, vocab)\n",
    "    reconstruction = batch_to_sentences(reconstruction, vocab)[0]\n",
    "    print(\"-- Original sentence: \\\"%s\\\"\" % random_sentence)\n",
    "    print(\"-- Model reconstruction: \\\"%s\\\"\" % reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2GpYHvqScSK"
   },
   "source": [
    "# Let's plot the training and validation statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYnp8E4MScSL"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps, training_ELBO = list(zip(*train_ELBOs))\n",
    "_, training_KL = list(zip(*train_KLs))\n",
    "_, val_ELBO = list(zip(*val_ELBOs))\n",
    "_, val_KL = list(zip(*val_KLs))\n",
    "epochs, val_ppl = list(zip(*val_perplexities))\n",
    "_, val_NLL = list(zip(*val_NLLs))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Plot training ELBO and KL\n",
    "ax1.set_title(\"Training ELBO\")\n",
    "ax1.plot(steps, training_ELBO, \"-o\")\n",
    "ax2.set_title(\"Training KL\")\n",
    "ax2.plot(steps, training_KL, \"-o\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation ELBO and KL\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax1.set_title(\"Validation ELBO\")\n",
    "ax1.plot(steps, val_ELBO, \"-o\", color=\"orange\")\n",
    "ax2.set_title(\"Validation KL\")\n",
    "ax2.plot(steps, val_KL, \"-o\",  color=\"orange\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation perplexities.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax1.set_title(\"Validation perplexity\")\n",
    "ax1.plot(epochs, val_ppl, \"-o\", color=\"orange\")\n",
    "ax2.set_title(\"Validation NLL\")\n",
    "ax2.plot(epochs, val_NLL, \"-o\",  color=\"orange\")\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF4L6yyKScSP"
   },
   "source": [
    "Let's load the best model according to validation perplexity and compute its perplexity on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K25svlmSScSQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ELBO (KL) = 101.68 (5.04) -- test perplexity = 447.85 -- test NLL = 111.31\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from disk.\n",
    "model = BowmanLM(vocab_size=vocab.size(), \n",
    "                 emb_size=emb_size, \n",
    "                 hidden_size=hidden_size, \n",
    "                 latent_size=latent_size, \n",
    "                 pad_idx=vocab[PAD_TOKEN],\n",
    "                 dropout=dropout)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "\n",
    "# Compute test perplexity and ELBO.\n",
    "test_perplexity, test_NLL = eval_perplexity(model, inference_model, test_dataset, vocab, \n",
    "                                            device, n_importance_samples)\n",
    "test_rec_loss, test_KL = eval_elbo(model, inference_model, test_dataset, vocab, device)\n",
    "test_ELBO = test_rec_loss - test_KL\n",
    "print(\"test ELBO (KL) = %.2f (%.2f) -- test perplexity = %.2f -- test NLL = %.2f\" % \n",
    "      (test_ELBO, test_KL, test_perplexity, test_NLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlxoPIa_ScSS"
   },
   "source": [
    "# Qualitative analysis\n",
    "\n",
    "Let's have a look at what how our trained model interacts with the learned latent space. First let's greedily decode some samples from the prior to assess the diversity of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x9siL9aScST"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: It 's a lot of the U.S. and the company 's largest business and the company 's largest business and the company 's largest business\n",
      "2: He said the company 's largest stock market is n't expected to be reached\n",
      "3: The companies are n't going to be a lot of the company 's largest business\n",
      "4: But the company 's largest business is n't expected to be a lot of the company 's largest stock market and the company 's stock market\n",
      "5: But the company 's results were n't disclosed\n",
      "6: Some analysts say the company 's stock market is n't disclosed\n",
      "7: In addition the company 's largest stock market was quoted at par to yield 7.93 %\n",
      "8: Still the company 's largest business is n't a good\n",
      "9: Total sales of the company 's largest stock market rose to yield %\n",
      "10: A spokesman said it was n't a good problem\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 samples from the standard normal prior.\n",
    "num_prior_samples = 10\n",
    "pz = Normal(torch.zeros(num_prior_samples, latent_size), \n",
    "            torch.ones(num_prior_samples, latent_size))\n",
    "z = pz.sample()\n",
    "z = z.to(device)\n",
    "\n",
    "# Use the greedy decoding algorithm to generate sentences.\n",
    "predictions = greedy_decode(model, z, vocab)\n",
    "predictions = batch_to_sentences(predictions, vocab)\n",
    "for num, prediction in enumerate(predictions):\n",
    "    print(\"%d: %s\" % (num+1, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7T4hLydScSV"
   },
   "source": [
    "Let's now have a look how good the model is at reconstructing sentences from the test dataset using the approximate posterior mean and a couple of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fi-g4ASScSW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \"Adding to unsteadiness in the industry seasonal retail ad spending patterns in newspapers have been upset by shifts in ownership and general hardships within the retail industry\"\n",
      "Posterior mean reconstruction: \"Traders said the company 's largest stock market is expected to be a new company 's largest stock market and the company 's largest stock market\"\n",
      "Posterior sample reconstruction (1): \"As a result of the company 's largest stock market is expected to be a new company 's largest stock market and the company said\"\n",
      "Posterior sample reconstruction (2): \"Traders said the company 's largest stock market is expected to be a new company 's largest stock market and the company 's largest stock market\"\n",
      "Posterior sample reconstruction (3): \"It 's a lot of the U.S. and the company 's largest business and the company 's largest business and the company 's largest business\"\n"
     ]
    }
   ],
   "source": [
    "# Pick a random test sentence.\n",
    "test_sentence = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x).\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_sentence], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "\n",
    "# Decode using the mean.\n",
    "z_mean = qz.mean()\n",
    "mean_reconstruction = greedy_decode(model, z_mean, vocab)\n",
    "mean_reconstruction = batch_to_sentences(mean_reconstruction, vocab)[0]\n",
    "\n",
    "print(\"Original: \\\"%s\\\"\" % test_sentence)\n",
    "print(\"Posterior mean reconstruction: \\\"%s\\\"\" % mean_reconstruction)\n",
    "\n",
    "# Decode a couple of samples from the approximate posterior.\n",
    "for s in range(3):\n",
    "    z = qz.sample()\n",
    "    sample_reconstruction = greedy_decode(model, z, vocab)\n",
    "    sample_reconstruction = batch_to_sentences(sample_reconstruction, vocab)[0]\n",
    "    print(\"Posterior sample reconstruction (%d): \\\"%s\\\"\" % (s+1, sample_reconstruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fi0THhQ5ScSZ"
   },
   "source": [
    "We can also qualitatively assess the smoothness of the learned latent space by interpolating between two sentences in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8Ai6FxnScSZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: \"Why not simply questions designed to identify sources and causes of waste and inefficiency\"\n",
      "(1-0.00) * qz1.mean + 0.00 qz2.mean: \"Still the company 's largest business is n't a lot of the company\"\n",
      "(1-0.25) * qz1.mean + 0.25 qz2.mean: \"Most of the U.S. and the company 's largest business is n't expected to be reached\"\n",
      "(1-0.50) * qz1.mean + 0.50 qz2.mean: \"The companies are n't expected to be a lot of the company 's largest stock market and the company said\"\n",
      "(1-0.75) * qz1.mean + 0.75 qz2.mean: \"The National Mortgage Association of the company 's largest stock market was priced at par to yield from 1.8470 billion yen from 1.17 billion\"\n",
      "(1-1.00) * qz1.mean + 1.00 qz2.mean: \"The National Mortgage Association of the company 's largest stock market rose to yield from 6.20 % in the third quarter ended Sept. 30\"\n",
      "Sentence 2: \"British Aerospace PLC and France 's Thomson-CSF S.A. said they are nearing an agreement to merge guided-missile divisions greatly expanding collaboration between the two defense contractors\"\n"
     ]
    }
   ],
   "source": [
    "# Pick a random test sentence.\n",
    "test_sentence_1 = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x).\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_sentence_1], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "qz_1 = qz.mean()\n",
    "\n",
    "# Pick a random second test sentence.\n",
    "test_sentence_2 = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x) again.\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_sentence_2], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "qz_2 = qz.mean()\n",
    "\n",
    "# Now interpolate between the two means and generate sentences between those.\n",
    "num_sentences = 5\n",
    "print(\"Sentence 1: \\\"%s\\\"\" % test_sentence_1)\n",
    "for alpha in np.linspace(start=0., stop=1., num=num_sentences):\n",
    "    z = (1-alpha) * qz_1 + alpha * qz_2\n",
    "    reconstruction = greedy_decode(model, z, vocab)\n",
    "    reconstruction = batch_to_sentences(reconstruction, vocab)[0]\n",
    "    print(\"(1-%.2f) * qz1.mean + %.2f qz2.mean: \\\"%s\\\"\" % (alpha, alpha, reconstruction))\n",
    "print(\"Sentence 2: \\\"%s\\\"\" % test_sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QnxO6X9c8Pc"
   },
   "source": [
    "Task from NLP Week: **Measure of the similarity of two texts**.\n",
    "\n",
    "1. Take random sentence from test dataset (IMO it is important to take sentences from test, not from train, because we are trying to understand how Inference Model works).\n",
    "\n",
    "2. Find the closest 10 sentences ($argmin KL(q(z|x_i) || q(z|x_a))$ by $x_i$). (In addition, will find the closest 10 sents by loc. I think it can be also interesting).\n",
    "\n",
    "3. How does $q1, q2$ should look like, to be true: $KL(q1||q2) < 1/2$, but $KL(q2||q1) > 2$\n",
    "\n",
    "4. Find in dataset such pairs (from 3 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_wsvY-W1NNz"
   },
   "source": [
    "Example of q1, q2:\n",
    "Template: q: (loc, scale)\n",
    "\n",
    "q1: ($\\sqrt 3$, 1)\n",
    "\n",
    "q2: (0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BniRHVXf1sID"
   },
   "source": [
    "**Explanation.** Formula for KL:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{KL}\\left(\\mathcal N(\\mu_1, \\sigma_1^2) || \\mathcal N(\\mu_2, \\sigma_2^2)\\right)\n",
    "&= \\frac{1}{2 \\sigma_2^2} \\left((\\mu_1 - \\mu_2)^2 + \\sigma_1^2 - \\sigma_2^2\\right) + \\log \\frac{\\sigma_2}{\\sigma_1}\n",
    "\\end{align}\n",
    "\n",
    "Set $\\frac{\\sigma_1}{\\sigma_2} = s;$ $(\\mu_1 - \\mu_2)^2 = k$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\\frac{1}{2s^2} + \\log(s) + \\frac{k}{2\\sigma^2_2} < 1$\n",
    "\n",
    "$\\frac{s^2}{2} - \\log(s) + \\frac{k}{2\\sigma^2_1} > 2.5$\n",
    "\n",
    "Let $s = 2$, then:\n",
    "\n",
    "$\\frac{k}{2\\sigma^2_2} < 1\\frac{1}{8} - \\log(2)$\n",
    "\n",
    "$\\frac{k}{2\\sigma^2_1} > 0.5 + \\log(2)$\n",
    "\n",
    "Let $k = 3$; $\\sigma_1 = 1$, then it will satisfy inequalities.\n",
    "\n",
    "$\\sigma_1 = 1; \\sigma_2 = 2, \\mu_2 = 0, \\mu_1 = \\sqrt 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLkKLAQe1nO1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "q1 = (math.sqrt(3), 1)\n",
    "q2 = (0, 2)\n",
    "\n",
    "def get_norm_x_y(q):\n",
    "    mu, sigma = q\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    y = stats.norm.pdf(x, mu, sigma)\n",
    "    return x, y\n",
    "\n",
    "plt.plot(*get_norm_x_y(q1), label='q1')\n",
    "plt.plot(*get_norm_x_y(q2), label='q2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_a idx: 988\n",
      "sent: \n",
      "  While far more restrained than the pace at the beginning of the year that is still a steeper rise than the 4.0 % increase for all of 1988\n"
     ]
    }
   ],
   "source": [
    "# 1 choose random idx\n",
    "x_a_idx = np.random.choice(len(test_dataset))\n",
    "print('x_a idx: {}'.format(x_a_idx))\n",
    "print('sent: \\n  {}'.format(test_dataset[x_a_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_internal_representation(sent):\n",
    "    x_in, _, seq_mask, seq_len = create_batch([sent], vocab, device)\n",
    "    qz = inference_model(x_in, seq_mask, seq_len)\n",
    "    return qz\n",
    "\n",
    "# 2.1 compute all internal representations and find 10 closest sents\n",
    "qzs = list(map(get_internal_representation, test_dataset))\n",
    "qza = qzs[x_a_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TePCKbnvem-P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By some accounts on Wall Street and in the industry the inventory reductions are near an end which may presage firmer demand\n",
      "Once again the specialists were not able to handle the imbalances on the floor of the New York Stock Exchange said Christopher Pedersen senior vice president at Twenty-First Securities Corp\n",
      "Of course we 've also got a judiciary that seeks the same objective\n",
      "But there were no buyers\n",
      "Some shareholders have held off until today because any fund exchanges made after Friday 's close would take place at today 's closing prices\n",
      "All these actions Mr. Atherton said will result in a loss of 125 million to 150 million for the third quarter\n",
      "We had a good amount of selling from institutions but not as much panic Mr. DaPuzzo said\n",
      "The 50 million warrants will be priced at 1.25 each and are expected to carry a premium to the share price of about 26 %\n",
      "While far more restrained than the pace at the beginning of the year that is still a steeper rise than the 4.0 % increase for all of 1988\n",
      "When Attack hit the shelves in 1987 P&G 's share of the Japanese market fell to about 8 % from more than 20 %\n"
     ]
    }
   ],
   "source": [
    "sorted_sents = sorted([(min(qz.kl(qza)[0]), sent) for qz, sent in zip(qzs, test_dataset)])\n",
    "for i in range(1, 11):  # 1 sent is x_a_idx sent\n",
    "    print(sorted_sents[i][1])  # kl with sentence"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SentVAE.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
