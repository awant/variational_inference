{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Udt3kHMdWvYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to import some helper code, so we need to run this"
      ]
    },
    {
      "metadata": {
        "id": "U8eXUCRiWvYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "nb_dir = os.path.split(os.getcwd())[0]\n",
        "if nb_dir not in sys.path:\n",
        "    sys.path.append(nb_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYhItDYMZi6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab\n",
        "\n",
        "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
      ]
    },
    {
      "metadata": {
        "id": "_shCMftIx1rW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "using_colab = True\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-fFME2OW22i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if using_colab:\n",
        "  !rm -fr dgm4nlp sst\n",
        "  !git clone https://github.com/probabll/dgm4nlp.git\n",
        "  !cp -R dgm4nlp/notebooks/sst ./  \n",
        "  !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_7NCZlZacNu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can start our lab."
      ]
    },
    {
      "metadata": {
        "id": "s9mH-rUhWvYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# CPU should be fine for this lab\n",
        "device = torch.device('cpu')  \n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "from collections import OrderedDict\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okMoxTJ9bWjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification \n",
        "\n",
        "\n",
        "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
        "\n",
        "\n",
        "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
        "\n",
        "\n",
        "\n",
        "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
        "\n",
        "\\begin{align}\n",
        "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
        "\n",
        "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
        "\n",
        "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
        "\n",
        "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
        "\\begin{align}\n",
        "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
        "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
        "\\end{align}\n",
        " to estimate $\\theta$ by maximisation:\n",
        " \\begin{align}\n",
        " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
        " \\end{align}\n",
        " \n",
        "\n",
        "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
        "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
        "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
        "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
        "\\end{align}\n",
        "\n",
        "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
        "\n",
        "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
        "\n",
        "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4LUjyO-39zan",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
      ]
    },
    {
      "metadata": {
        "id": "4z8Bt5no9z6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "print(\"Loading data\")\n",
        "train_data = list(examplereader('sst/data/sst/train.txt'))\n",
        "dev_data = list(examplereader('sst/data/sst/dev.txt'))\n",
        "test_data = list(examplereader('sst/data/sst/test.txt'))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))\n",
        "\n",
        "print('\\n# Examples')\n",
        "example = dev_data[0]\n",
        "print(\"First dev example:\", example)\n",
        "print(\"First dev example tokens:\", example.tokens)\n",
        "print(\"First dev example label:\", example.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB2lEsNuWvYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "\n",
        "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
        "\n",
        "**Embedding Layer**\n",
        "\n",
        "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
        "\n",
        "We will denote the embedding of the $i$th word of the input by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf x_i = \\text{glove}(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "**Encoder Layer**\n",
        "\n",
        "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
        "\\end{equation}\n",
        "\n",
        "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
        "\n",
        "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
        "\n",
        "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
        "\n",
        "\\begin{align}\n",
        "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
      ]
    },
    {
      "metadata": {
        "id": "Kc15Nv2i41cq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "To leave an indication of the shape of tensors in the code, we use the following convention\n",
        "\n",
        "```python\n",
        "[B, T, D]\n",
        "```\n",
        "\n",
        "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
        "\n",
        "\n",
        "Consider the following abstract Encoder class:"
      ]
    },
    {
      "metadata": {
        "tags": [
          "encoders"
        ],
        "id": "xwEPXT2MWvYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    An Encoder for us is a function that\n",
        "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
        "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
        "      \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "    def forward(self, inputs, mask, lengths):\n",
        "        \"\"\"\n",
        "        The input is a batch-first tensor of token ids. Here is an example:\n",
        "        \n",
        "        Example of inputs (though rather than words, we have word ids):\n",
        "            INPUTS                     MASK       LENGTHS\n",
        "            [the nice cat -PAD-]    -> [1 1 1 0]  [3]\n",
        "            [the nice dog running]  -> [1 1 1 1]  [4]\n",
        "            \n",
        "        Note that:\n",
        "              mask =  inputs == 1\n",
        "              lengths = mask.sum(dim=-1)\n",
        "        \n",
        "        :param inputs: [B, T, I]\n",
        "        :param mask: [B, T]\n",
        "        :param lengths: [B]\n",
        "        :returns: [B, T, O], [B, O]\n",
        "            where the first tensor is the transformed input\n",
        "            and the second tensor is a summary of all inputs\n",
        "        \"\"\"\n",
        "        pass\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA5wmkcRg9Am",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start easy, implement a *bag of words* encoder:"
      ]
    },
    {
      "metadata": {
        "id": "U-9hLQ0lF5SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BagOfWordsEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This encoder does not transform the input sequence, \n",
        "     and its summary output is just a sum.\n",
        "    \"\"\"\n",
        "    \n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IS7x0hLrUXfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also consider implementing\n",
        "\n",
        "* a feed-forward encoder with average pooling\n",
        "* and a biLSTM encoder\n",
        "\n",
        "but these are certainly optional."
      ]
    },
    {
      "metadata": {
        "id": "BpOGFpK_Uo0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FFEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    A typical feed-forward NN with tanh hidden activations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, output_size, \n",
        "                 activation=None, \n",
        "                 hidden_sizes=[], \n",
        "                 aggregator='avg',\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "        :param output_size: int\n",
        "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
        "        :param aggregator: 'sum' or 'avg'\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        :param x: sequence of word embeddings, shape [B, T, I]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return: \n",
        "            outputs [B, T, O]\n",
        "            sum/avg pooling [B, O]\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxQ5djZ_VAvK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class LSTMEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This module encodes a sequence using a bidirectional LSTM\n",
        "     it returns the final state\n",
        "     and the hidden states at each time step. Note: we concatenate representations\n",
        "     from the two directions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, \n",
        "                 hidden_size: int = 200,\n",
        "                 batch_first: bool = True,\n",
        "                 bidirectional: bool = True):\n",
        "        \"\"\"\n",
        "        :param in_features:\n",
        "        :param hidden_size:\n",
        "        :param batch_first:\n",
        "        :param bidirectional:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Encode sentence x\n",
        "        :param x: sequence of word embeddings, shape [B, T, I]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return:\n",
        "            outputs [B, T, O]\n",
        "            final state [B, O]\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_zz5zIyVkSh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is some helper code to select and return an encoder:"
      ]
    },
    {
      "metadata": {
        "id": "59ZU6JddVjMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
        "    \"\"\"Returns the requested layer.\"\"\"\n",
        "\n",
        "    # TODO: make pass and average layers\n",
        "    if layer == \"bow\":\n",
        "        return BagOfWordsEncoder()\n",
        "    elif layer == 'ff':\n",
        "        return FFEncoder(\n",
        "            in_features, \n",
        "            2 * hidden_size,   # for convenience\n",
        "            hidden_sizes=[hidden_size], \n",
        "            aggregator='avg')\n",
        "    elif layer == \"lstm\":\n",
        "        return LSTMEncoder(\n",
        "            in_features, \n",
        "            hidden_size,\n",
        "            bidirectional=bidirectional)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown layer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY8LZiMN5CHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification with Latent Rationale\n",
        "\n",
        "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
        "\n",
        "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
        "\n",
        "The picture below depicts our latent-variable model for rationale extraction:\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
        "\n",
        "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
        "\n",
        "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
        "\n",
        "\\begin{align}\n",
        "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
        "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
        "\n",
        "\n",
        "Here is an example design for $f$:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
        "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
        "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
        "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hDHNxLHMWvY-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prior\n",
        "\n",
        "\n",
        "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
        "\n",
        "\\begin{align}\n",
        "Z_i & \\sim \\text{Bern}(p_1)\n",
        "\\end{align}\n",
        "\n",
        "whose pmf is $\\text{Bern}(z_i|p_1) = p_1^{z_i}\\times (1-p_1)^{1-z_i}$.\n",
        "\n",
        "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
      ]
    },
    {
      "metadata": {
        "id": "iCBcHnTsOuDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Bernoulli:\n",
        "    \"\"\"\n",
        "    This class encapsulates a collection of Bernoulli distributions. \n",
        "    Each Bernoulli is uniquely specified by p_1, where\n",
        "        Bernoulli(X=x|p_1) = pow(p_1, x) * pow(1 - p_1, 1 - x)\n",
        "    is the Bernoulli probability mass function (pmf). \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, logits=None, probs=None):\n",
        "        \"\"\"\n",
        "        We can specify a Bernoulli distribution via a logit or a probability. \n",
        "         You need to specify at least one, and if you specify both, beware that\n",
        "         in this implementation logits will be used.\n",
        "         \n",
        "        Recall that: probs = sigmoid(logits).\n",
        "         \n",
        "        :param logits: a tensor of logits (a logit is defined as log (p_1/p_0))\n",
        "            where p_0 = 1 - p_1\n",
        "        :param probs: a tensor of probabilities, each in (0, 1)\n",
        "        \n",
        "        \"\"\"        \n",
        "        pass\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Returns a single sample with the same shape as the parameters\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        Assess the log probability of a sample. \n",
        "        \n",
        "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
        "        :returns: tensor with log probabilities with the same shape as parameters\n",
        "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def kl(self, other: 'Bernoulli'):\n",
        "        \"\"\"\n",
        "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
        "        \n",
        "        :return: KL[self||other] with same shape parameters\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0yfkCZlWvZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier\n",
        "\n",
        "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
        "\n",
        "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
      ]
    },
    {
      "metadata": {
        "id": "F-6JLDnBQcdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Categorical:\n",
        "    \n",
        "    def __init__(self, log_probs):\n",
        "        # [B, K]: class probs\n",
        "        self.log_probs = log_probs\n",
        "        \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [B] integers (targets)\n",
        "        :returns: [B] scalars (log probabilities)\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdrM_YRI8xBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "and a classifier architecture:\n",
        "\n",
        "* implement the forward method"
      ]
    },
    {
      "metadata": {
        "id": "sz7GaKbgRCd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:        nn.Embedding = None,\n",
        "                 hidden_size:  int = 200,\n",
        "                 output_size:  int = 1,\n",
        "                 dropout:      float = 0.1,\n",
        "                 layer:        str = \"pass\",\n",
        "                 ):\n",
        "\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        enc_size = hidden_size * 2\n",
        "        # Here we embed the words\n",
        "        self.embed_layer = nn.Sequential(\n",
        "            embed\n",
        "            # , nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "\n",
        "        # and here we predict categorical parameters\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(enc_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask, z) -> Categorical:\n",
        "        \"\"\"\n",
        "        :params x: [B, T, I] word representations\n",
        "        :params mask: [B, T] indicates valid positions\n",
        "        :params z: [B, T] binary selectors\n",
        "        :returns: one Categorical distribution per instance in the batch\n",
        "          each conditioning only on x_i for which z_i = 1\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2waCCBF9MaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "\n",
        "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
        "\n",
        "\\begin{align}\n",
        "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
        "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
        "\n",
        "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
        "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "0jcVdYTg8Wun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
        "\\begin{align}\n",
        "Q(z|x, y, \\lambda) \n",
        "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
        "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
        "\\end{align}\n",
        "\n",
        "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
        "\n",
        "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
        "\n",
        "Here is an example design for $g$:\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
        "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
        "\\end{align}\n",
        "where\n",
        "* $\\text{glove}$ is a pre-trained embedding function;\n",
        "* $\\text{dense}_1$ is a dense layer with a single output;\n",
        "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
        "\n",
        "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
        "\n",
        "Here we implement this product of Bernoulli distributions:\n",
        "\n",
        "* implement $g$ in the constructor \n",
        "* and the forward pass"
      ]
    },
    {
      "metadata": {
        "id": "YLxfcAbuSiFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProductOfBernoullis(nn.Module):\n",
        "    \"\"\"\n",
        "    This is an inference network that parameterises independent Bernoulli distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:       nn.Embedding,\n",
        "                 hidden_size: int = 200,\n",
        "                 layer:       str = \"bow\"\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param embed: an embedding layer\n",
        "        :param hidden_suze: hidden size for transformed inputs\n",
        "        :param layer: 'bow' for BoW encoding\n",
        "          you may alternatively implement and 'lstm' option\n",
        "          which uses a biLSTM to transform the inputs         \n",
        "        \"\"\"\n",
        "        super(ProductOfBernoullis, self).__init__()\n",
        "        # 1. we should have an embedding layer \n",
        "        # 2. we may transform the representations\n",
        "        # 3. and we should compute parameters for Bernoulli distributions\n",
        "        pass\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask) -> Bernoulli:\n",
        "        \"\"\"\n",
        "        It takes a tensor of tokens (integers)\n",
        "         and predicts a Bernoulli distribution for each position.\n",
        "        \n",
        "        :param x: [B, T]\n",
        "        :param mask: [B, T]\n",
        "        :returns: Bernoulli\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcCu7vkKWvZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n",
        "\n",
        "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
        "\n",
        "\\begin{align}\n",
        "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
        "\\text{ELBO}&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
        "\\end{align}\n",
        "\n",
        "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
        "\n",
        "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
        "\n",
        "### Gradient of the classifier network\n",
        "\n",
        "For the classifier, we encounter no problem:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\text{ELBO} &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{\\color{blue}{0}}  \\\\\n",
        "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
        "\\end{align}\n",
        "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
        "\n",
        "\n",
        "### Gradient of the inference network\n",
        "\n",
        "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\lambda \\text{ELBO} &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
        "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
        "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))  \n",
        "\\end{align}\n",
        "\n",
        "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
      ]
    },
    {
      "metadata": {
        "id": "6cdfkOYdC0LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
        "\n",
        "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|\\phi))\n",
        "\\end{align}\n",
        "where we introduce an auxiliary function such that\n",
        "\\begin{align}\n",
        "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
        "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
        "\\end{align}\n",
        "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
        "\n",
        "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
      ]
    },
    {
      "metadata": {
        "id": "FednEChaX6WI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal S(\\theta, \\lambda|x, y) = \\color{red}{?}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y) = \\color{red}{?}\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "OaUMKDShx9T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implement the forward pass and loss below:"
      ]
    },
    {
      "metadata": {
        "id": "Cnwwk-7tfR02",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    \n",
        "    Classifier model:\n",
        "        Z_i ~ Bern(p_1) for i in 1..n\n",
        "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
        "    \n",
        "    Inference model:\n",
        "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
        "            where b_i = g_i(x)\n",
        "    \n",
        "    Objective:\n",
        "        Single-sample MC estimate of ELBO\n",
        "    \n",
        "    Loss: \n",
        "        Surrogate loss\n",
        "\n",
        "    Consists of:\n",
        "        - a product of Bernoulli distributions inference network\n",
        "        - a classifier network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab:       object = None,\n",
        "                 vocab_size:  int = 0,\n",
        "                 emb_size:    int = 200,\n",
        "                 hidden_size: int = 200,\n",
        "                 num_classes: int = 5,\n",
        "                 prior_p1:    float = 0.3,                 \n",
        "                 det_prior: bool = True,\n",
        "                 beta_shape:  list = [0.6, 0.6],\n",
        "                 dropout:     float = 0.1,\n",
        "                 layer_cls:   str = 'bow',\n",
        "                 layer_inf:   str = 'bow',\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param vocab: Vocabulary\n",
        "        :param vocab_size: necessary for embedding layer\n",
        "        :param emb_size: dimensionality of embedding layer\n",
        "        :param hidden_size: dimensionality of hidden layers\n",
        "        :param num_classes: number of classes\n",
        "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
        "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
        "        :param beta_shape: (pair of positive scalars) \n",
        "            when the prior parameter is stochastic\n",
        "            it is sampled from a Beta distribution (ignore this at first)\n",
        "        :param dropout: (scalar) dropout rate\n",
        "        :param layer_cls: type of encoder for classification\n",
        "        :param layer_inf: type of encoder for inference\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
        "\n",
        "        self.cls_net = Classifier(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size, \n",
        "            output_size=num_classes,\n",
        "            dropout=dropout, \n",
        "            layer=layer_cls)\n",
        "        \n",
        "        self.inference_net = ProductOfBernoullis(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size,\n",
        "            layer=layer_inf)\n",
        "        \n",
        "        self._prior_p1 = prior_p1\n",
        "        self._det_prior = det_prior\n",
        "        self._beta_shape = beta_shape\n",
        "        \n",
        "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
        "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
        "        if self._det_prior:\n",
        "            return self._prior_p1\n",
        "        else:\n",
        "            a, b = self._beta_shape\n",
        "            prior_p1 = np.random.beta(a, b)\n",
        "            prior_p1 = max(prior_p1, p_min)\n",
        "            prior_p1 = min(prior_p1, p_max)\n",
        "        return prior_p1\n",
        "\n",
        "    def predict(self, py: Categorical, **kwargs):\n",
        "        \"\"\"\n",
        "        Predict deterministically using argmax.\n",
        "        :param py: B Categorical distributions (one per instance in batch)\n",
        "        :return: predictions\n",
        "            [B] sentiment levels\n",
        "        \"\"\"\n",
        "        assert not self.training, \"should be in eval mode for prediction\"\n",
        "        return py.log_probs.argmax(-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Generate a sequence z with inference model, \n",
        "         then predict with rationale xz, that is, x masked by z.\n",
        "\n",
        "        :param x: [B, T] documents        \n",
        "        :param mask: [B, T] indicates valid positions vs padded positions\n",
        "        :return: \n",
        "            Categorical distributions P(y|x, z)\n",
        "            Bernoulli distributions Q(z|x)\n",
        "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_loss(self,                   \n",
        "                 y, \n",
        "                 py: Categorical,\n",
        "                 qz: Bernoulli, \n",
        "                 z, \n",
        "                 mask,\n",
        "                 iter_i=0, \n",
        "                 # you may ignore the rest of the arguments for the time being\n",
        "                 #  leave them as they are\n",
        "                 kl_weight=1.0,\n",
        "                 min_kl=0.0,\n",
        "                 ll_mean=0.,\n",
        "                 ll_std=1.,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        This computes the loss for the whole model.\n",
        "\n",
        "        :param y: target labels [B]\n",
        "        :param py: conditionals P(y|x, z)\n",
        "        :param qz: approximate posteriors Q(z|x)\n",
        "        :param z: sample of binary selectors [B, T]\n",
        "        :param mask: indicates valid positions [B, T]\n",
        "        :param iter_i: indicates the iteration\n",
        "        :param kl_weight: (scalar) multiplies the KL term\n",
        "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
        "        :param ll_mean: (scalar) running average of reward\n",
        "        :param ll_std: (scalar) running standard deviation of reward\n",
        "        :return: loss (torch node), terms (dict)\n",
        "        \n",
        "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
        "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
        "            \n",
        "            Consider tracking the following:\n",
        "            Single-sample ELBO: terms['elbo']\n",
        "            Log-Likelihood log P(y|x,z): terms['ll']\n",
        "            KL: terms['kl']\n",
        "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
        "            Rate of selected words: terms['selected']\n",
        "        \"\"\"\n",
        "\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNQDXTpqWvZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This will be used later for maintaining runnin averages of quantites like \n",
        "#  terms in the ELBO\n",
        "from collections import deque\n",
        "\n",
        "class MovingStats:\n",
        "    \n",
        "    def __init__(self, memory=-1):\n",
        "        self.data = deque([])\n",
        "        self.memory = memory\n",
        "        \n",
        "    def append(self, value):\n",
        "        if self.memory != 0:\n",
        "            if self.memory > 0 and len(self.data) == self.memory:\n",
        "                self.data.popleft()\n",
        "            self.data.append(value)\n",
        "        \n",
        "    def mean(self):\n",
        "        if len(self.data):\n",
        "            return np.mean([x for x in self.data])\n",
        "        else:\n",
        "            return 0.\n",
        "    \n",
        "    def std(self):\n",
        "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "081YSfU9WvZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ]
    },
    {
      "metadata": {
        "id": "9Pc80gseWvZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# some helper code for mini batching\n",
        "#  this will take care of annoying things such as \n",
        "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
        "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WVr97kilIRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim\n",
        "# We will use Adam\n",
        "from torch.optim import Adam\n",
        "# and a couple of tricks to reduce learning rate on plateau\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# here is some helper code to evaluate your model\n",
        "from sst.evaluate import evaluate\n",
        "\n",
        "\n",
        "cfg = dict()\n",
        "\n",
        "# Data\n",
        "cfg['training_path'] = \"sst/data/sst/train.txt\"\n",
        "cfg['dev_path'] = \"sst/data/sst/dev.txt\"\n",
        "cfg['test_path'] = \"sst/data/sst/test.txt\"\n",
        "cfg['word_vectors'] = 'sst/data/sst/glove.840B.300d.filtered.txt'\n",
        "# Model\n",
        "cfg['prior_p1'] = 0.3\n",
        "cfg['beta_a'] = 0.6\n",
        "cfg['beta_b'] = 0.6\n",
        "cfg['det_prior'] = True\n",
        "# Architecture\n",
        "cfg['num_epochs'] = 50\n",
        "cfg['print_every'] = 100\n",
        "cfg['eval_every'] = -1\n",
        "cfg['batch_size'] = 25\n",
        "cfg['eval_batch_size'] = 25\n",
        "cfg['subphrases'] = False\n",
        "cfg['min_phrase_length'] = 2\n",
        "cfg['lowercase'] = True\n",
        "cfg['fix_emb'] = True\n",
        "cfg['embed_size'] = 300\n",
        "cfg['hidden_size'] = 150\n",
        "cfg['num_layers'] = 1\n",
        "cfg['dropout'] = 0.5\n",
        "cfg['layer_inf'] = 'bow'\n",
        "cfg['layer_cls'] = 'bow'\n",
        "cfg['save_path'] = 'data/results'\n",
        "cfg['baseline_memory'] = 1000\n",
        "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
        "cfg['kl_weight'] = 1.  # start from zero to enable annealing\n",
        "cfg['kl_inc'] = 0.00001  \n",
        "# Optimiser (leave as is)\n",
        "cfg['lr'] = 0.0002\n",
        "cfg['weight_decay'] = 1e-5\n",
        "cfg['lr_decay'] = 0.5\n",
        "cfg['patience'] = 5\n",
        "cfg['cooldown'] = 5\n",
        "cfg['threshold'] = 1e-4\n",
        "cfg['min_lr'] = 1e-5\n",
        "cfg['max_grad_norm'] = 5.\n",
        "\n",
        "\n",
        "print('# Configuration')\n",
        "for k, v in cfg.items():\n",
        "    print(\"{:20} : {:10}\".format(k, v))\n",
        "\n",
        "\n",
        "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
        "\n",
        "if cfg[\"eval_every\"] == -1:\n",
        "    eval_every = iters_per_epoch\n",
        "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "print(\"Loading data\")\n",
        "train_data = list(examplereader(\n",
        "    cfg['training_path'],\n",
        "    lower=cfg['lowercase'], \n",
        "    subphrases=cfg['subphrases'],\n",
        "    min_length=cfg['min_phrase_length']))\n",
        "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
        "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))\n",
        "\n",
        "print('\\n# Example')\n",
        "example = dev_data[0]\n",
        "print(\"First dev example:\", example)\n",
        "print(\"First dev example tokens:\", example.tokens)\n",
        "print(\"First dev example label:\", example.label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "6PMqtVj0WvZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a vocabulary object to map str <-> int\n",
        "    vocab = Vocabulary()  # populated by load_glove\n",
        "    glove_path = cfg[\"word_vectors\"]\n",
        "    vectors = load_glove(glove_path, vocab)\n",
        "\n",
        "    # You may consider using tensorboardX\n",
        "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
        "\n",
        "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
        "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
        "\n",
        "\n",
        "    print('\\n# Constructing model')\n",
        "    model = Model(\n",
        "        vocab_size=len(vocab.w2i), \n",
        "        emb_size=cfg[\"embed_size\"],\n",
        "        hidden_size=cfg[\"hidden_size\"], \n",
        "        num_classes=len(t2i),\n",
        "        prior_p1=cfg['prior_p1'],\n",
        "        det_prior=cfg['det_prior'],\n",
        "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
        "        vocab=vocab, \n",
        "        dropout=cfg[\"dropout\"], \n",
        "        layer_cls=cfg[\"layer_cls\"],\n",
        "        layer_inf=cfg[\"layer_inf\"])\n",
        "\n",
        "    print('\\n# Loading embeddings')\n",
        "    with torch.no_grad():\n",
        "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "        if cfg[\"fix_emb\"]:\n",
        "            print(\"fixed word embeddings\")\n",
        "            model.embed.weight.requires_grad = False\n",
        "        model.embed.weight[1] = 0.  # padding zero\n",
        "\n",
        "        \n",
        "    # Congigure optimiser\n",
        "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
        "                     weight_decay=cfg[\"weight_decay\"])\n",
        "    # and learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
        "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
        "        min_lr=cfg[\"min_lr\"])\n",
        "\n",
        "    # Prepare a few auxiliary variables\n",
        "    iter_i = 0\n",
        "    train_loss = 0.\n",
        "    print_num = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    best_eval = 1.0e9\n",
        "    best_iter = 0\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Some debugging info\n",
        "    print(model)\n",
        "    print_parameters(model)\n",
        "\n",
        "    batch_size = cfg['batch_size']\n",
        "    eval_batch_size = cfg['eval_batch_size']\n",
        "    print_every = cfg['print_every']\n",
        "\n",
        "    # Parameters of tricks to better optimise the ELBO \n",
        "    kl_inc = cfg['kl_inc']\n",
        "    kl_weight = cfg['kl_weight']\n",
        "    min_kl = cfg['min_kl']\n",
        "    # Running estimates for baselines\n",
        "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
        "\n",
        "    while True:  # when we run out of examples, shuffle and continue\n",
        "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
        "\n",
        "            epoch = iter_i // iters_per_epoch\n",
        "            if epoch > cfg['num_epochs']:\n",
        "                break\n",
        "\n",
        "            # forward pass\n",
        "            model.train()\n",
        "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
        "            \n",
        "            mask = (x != 1)\n",
        "            py, qz, z = model(x, mask)\n",
        "\n",
        "            # \"KL annealing\"\n",
        "            kl_weight += kl_inc\n",
        "            if kl_weight > 1.:\n",
        "                kl_weight = 1.0\n",
        "                \n",
        "            loss, terms = model.get_loss(\n",
        "                y,\n",
        "                py=py, \n",
        "                qz=qz,\n",
        "                z=z,\n",
        "                mask=mask, \n",
        "                kl_weight=kl_weight,\n",
        "                min_kl=min_kl,\n",
        "                ll_mean=ll_moving_stats.mean(),\n",
        "                ll_std=ll_moving_stats.std(),\n",
        "                iter_i=iter_i)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            # keep an running estimate of the reward (log P(y|x,z))\n",
        "            ll_moving_stats.append(terms['ll'])\n",
        "\n",
        "            # backward pass\n",
        "            model.zero_grad()  # erase previous gradients\n",
        "\n",
        "            loss.backward()  # compute new gradients\n",
        "\n",
        "            # gradient clipping generally helps\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
        "\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            print_num += 1\n",
        "            iter_i += 1\n",
        "\n",
        "            # print info\n",
        "            if iter_i % print_every == 0:\n",
        "\n",
        "                train_loss = train_loss / print_every\n",
        "\n",
        "                print_str = make_kv_string(terms)\n",
        "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
        "                      (epoch, iter_i, train_loss, print_str))\n",
        "                losses.append(train_loss)\n",
        "                print_num = 0\n",
        "                train_loss = 0.\n",
        "\n",
        "            # evaluate\n",
        "            if iter_i % eval_every == 0:\n",
        "\n",
        "                dev_eval, rationales = evaluate(\n",
        "                    model, dev_data, \n",
        "                    batch_size=eval_batch_size, \n",
        "                    device=device,\n",
        "                    cfg=cfg, iter_i=iter_i)\n",
        "                accuracies.append(dev_eval[\"acc\"])\n",
        "\n",
        "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
        "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
        "                \n",
        "                for exid in range(3):\n",
        "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
        "                          ' '.join(rationales[exid][0]))\n",
        "                print()\n",
        "\n",
        "                # adjust learning rate\n",
        "                scheduler.step(dev_eval[\"loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "A5uYKcw-WvZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7w_Ko657vRGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Variance reduction\n",
        "\n",
        "**This is an extra**\n",
        "\n",
        "We can use a *control variate* to reduce the variance of our gradient estimates.\n",
        "\n",
        "Let's recap the idea in general terms. We are looking to solve some expectation\n",
        "\\begin{align}\n",
        "\\mu_f = \\mathbb E[f(Z)]\n",
        "\\end{align}\n",
        "but unfortunatelly, realising the full sum (or integral for continuous variables) is intractable. Thus we employ MC estimation\n",
        "\\begin{align}\n",
        "\\hat \\mu_f &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S f(z_s) & \\text{where }z_s \\sim Q(z|x)\n",
        "\\end{align}\n",
        "Note that the variance of this estimate is\n",
        "\\begin{align}\n",
        "\\text{Var}(\\hat \\mu_f) &=  \\frac{1}{S}\\text{Var}(f(Z)) \\\\\n",
        "&= \\frac{1}{S} \\mathbb E[( f(Z) - \\mathbb E[f(Z)])^2]\n",
        "\\end{align}\n",
        "Note that this variance is such that it goes down as we sample more, in a rate $\\mathcal O(S^{-1})$.\n",
        "See that if we sample $10$ times more, we will only obtain an decrease in variance in the order of $10^{-1}$. This means that sampling more is generally not the most convenient way to decrease variance.\n",
        "\n",
        "*Digression* we can estimate the variance itself via MC, an unbiased estimate looks like\n",
        "\\begin{align}\n",
        "\\hat \\sigma^2_f = \\frac{1}{S(S-1)} \\sum_{s=1}^S (f(z_s) - \\hat \\mu_f)^2\n",
        "\\end{align}\n",
        "but not that this estimate is even hard to improve since it decreases with $\\mathcal O(S^{-2})$.\n",
        "\n",
        "Back to out main problem: let's try and improve the variance of our estimator to $\\mu_f$.\n",
        "\n",
        "It's a fact, and it can be shown trivially, that\n",
        "\\begin{align}\n",
        "\\mu_f &=  \\mathbb E[f(Z) - \\psi(Z)] + \\underbrace{\\mathbb E[\\psi(Z)]}_{\\mu_\\psi} \\\\\n",
        " &\\overset{\\text{MC}}{\\approx} \\underbrace{\\left(\\frac{1}{S} \\sum_{s=1}^S f(z_s) - \\psi(z_s) \\right) + \\mu_\\psi}_{\\hat c}\n",
        "\\end{align}\n",
        "where we assume the existence of some function $\\psi(z)$ for which the expected value $\\mu_\\psi$ is known and we estimate the expected difference $\\mathbb E[f(Z) - \\psi(Z)]$ via MC. We used this axuxiliary function, also known as a *control variate*, to derive a new estimator, which we will denote by $\\hat c$.\n",
        "\n",
        "The variance of this new estimator is show below:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Var}( \\hat c ) &= \\text{Var}(\\hat \\mu_{f-\\psi}) + 2\\underbrace{\\text{Cov}(\\hat \\mu_{f-\\psi}, \\mu_\\psi)}_{\\mathbb E[\\hat \\mu_{f-\\psi}  \\mu_\\psi] - \\mathbb E[\\hat \\mu_{f-\\psi}] \\mathbb E[\\mu_\\psi]} + \\underbrace{\\text{Var}(\\mu_\\psi)}_{\\color{blue}{0} } \\\\\n",
        "&= \\frac{1}{S}\\text{Var}(f- \\psi)  + 2 \\underbrace{\\left( \\mu_\\psi \\mu_{f-\\psi} - \\mu_{f-\\psi} \\mu_\\psi \\right)}_{\\color{blue}{0}} \n",
        "\\end{align}\n",
        "where the variance of $\\mu_\\psi$ is 0 because we know it in closed form (no need for MC estimation), and the covariance is $0$ as shown in the second row.\n",
        "\n",
        "That is, the variance of $\\hat c$ is essentially the variance of estimating $\\mathbb E[f(Z) - \\psi(Z)]$, which in turn depends on the variance \n",
        "\n",
        "\\begin{align}\n",
        "\\text{Var}(f-\\psi) &= \\text{Var}(f) - 2\\text{Cov}(f, \\psi) + \\text{Var}(\\psi)\n",
        "\\end{align}\n",
        "where we can see that if $\\text{Cov}(f, \\psi) > \\frac{\\text{Var}(\\psi)}{2}$ we achieve variance reduction as then $\\text{Var}(f-\\psi)$ would be smaller than $\\text{Var(f)}$.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ovKcRnqH_PGp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Baselines\n",
        "\n",
        "Baslines are control variates of a very simple form:\n",
        "\\begin{align}\n",
        "\\mathbb E[f(Z)] = \\mathbb E[f(Z) - C] + \\mathbb E[C]\n",
        "\\end{align}\n",
        "where $C$ is a constant with respect to $z$.\n",
        "\n",
        "In the context of the score function estimator, a baseline looks like a quantity $C(x; \\omega)$, this may be\n",
        "* just a constant;\n",
        "* or a function of the input (but not of the latent variable), which could be itself implemented as a neural network;\n",
        "* a combination of the two.\n",
        " \n",
        "\n",
        "Let's focus on the first term of the ELBO (so I'm omitting the KL term here). The gradient with respect to parameters of the inference model becomes:\n",
        "\n",
        "\\begin{align}\n",
        "&\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\\\\\n",
        "&=\\mathbb E_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) - \\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] + \\underbrace{\\mathbb E_{Q(z|x, \\lambda)}\\left[\\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] }_{=0} \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\color{blue}{\\left(\\log P(x|z, \\theta) - C(x; \\omega) \\right)}\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right] \\\\\n",
        "&\n",
        "\\end{align}\n",
        "We can show that the last term is $0$\n",
        "\n",
        "\\begin{align}\n",
        "&\\mathbb E_{Q(z|x, \\lambda)}\\left[C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]  \\\\&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]\\\\\n",
        "&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\right] \\\\\n",
        "&= C(x; \\omega) \\sum_z Q(z|x, \\lambda) \\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\\\\n",
        "&= C(x; \\omega) \\sum_z\\nabla_\\lambda Q(z|x, \\lambda)  \\\\\n",
        "&= C(x; \\omega) \\nabla_\\lambda \\underbrace{\\sum_z Q(z|x, \\lambda)  }_{=1}\\\\\n",
        "&=0\n",
        "\\end{align}\n",
        "\n",
        "Examples of useful baselines:\n",
        "\n",
        "* a running average of the learning signal: at some iteration $t$ we can use a running average of $\\log P(x|z, \\theta)$ using parameter estimates $\\theta$ from iterations $i < t$, this is a baseline that likely leads to high correlation between control variate and learning signal and can lead to variance reduction;\n",
        "* another technique is to have an MLP with parameters $\\omega$ predict a scalar and train this MLP to approximate the learning signal $\\log P(x|z, \\theta)$ via regression:\n",
        "\\begin{align}\n",
        "\\arg\\max_\\omega \\left( C(x; \\omega) - \\log P(x|z, \\theta) \\right)^2\n",
        "\\end{align}\n",
        "its left as an extra to implement these ideas.\n",
        "\n",
        "One more note: we can also use something called a *multiplicative baseline* in the literature of reinforcement learning, whereby we incorporate a running estimate of the standard deviation of the learning signal computed based on the values attained on previous iterations:\n",
        "\\begin{align}\n",
        "\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\frac{1}{\\hat\\sigma_{\\text{past}}}\\left(\\log P(x|z, \\theta) - \\hat \\mu_{\\text{past}}\\right)\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\n",
        "\\end{align}\n",
        "this form of contorl variate aim at promoting the learning signal (or reward in reinforcement learning literature) to be distributed by $\\mathcal N(0, 1)$. Note that multiplying the reward by a constant does not bias the estimator, and in this case, may lead to variance reduction."
      ]
    },
    {
      "metadata": {
        "id": "SVsWgmlIWvZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2VntYV3WvZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTxG1AvPWvZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}